{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc513301-e3eb-4adc-905b-39dd6c3034f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #catalogue\n",
    "#1) global_alignment\n",
    "#2) traceback\n",
    "#3) match_fn\n",
    "#4) global_alignment_matrix\n",
    "#5) count_kmers1\n",
    "#6) decapeptide_filter_query\n",
    "#7) decapeptide_filter\n",
    "#8) ktuple_filter\n",
    "#9) import_df_by_part2\n",
    "#10) import_df_by_part1\n",
    "#11) import_df_by_part\n",
    "#12) substring_occurrences\n",
    "#13) search_polyA\n",
    "#14) generate_impure_pA\n",
    "#15) find_tr_ORF_stats\n",
    "#16) which_frame\n",
    "#17) which_frame_by_coord\n",
    "#18) flnc_tr_seq_stats\n",
    "#19) prot_properties\n",
    "#20) rev_comp\n",
    "#21) translate\n",
    "#22) read_blast\n",
    "#23) calc_overlaps\n",
    "#24) overlaps_to_plot\n",
    "#25) gtf_to_plot\n",
    "#26) parse_fasta\n",
    "#27) write_fasta_from_fasta_df\n",
    "#28) write_fasta_from_MREdf\n",
    "#29) write_fasta_from_overlap_df1\n",
    "#30) uniques_only\n",
    "#31) uniques_only_list\n",
    "#32) write_list\n",
    "#33) read_list\n",
    "#34) find_fltc_ORFs\n",
    "#35) find_ORFs_old\n",
    "#36) list_duplicates\n",
    "#37) fasta_subseqs\n",
    "#38) calc_nucl_freq\n",
    "#39) calc_aa_freq\n",
    "#40) count_aas\n",
    "#41) DNA_onehot\n",
    "#42) count_kmers\n",
    "#43) find_overrepresented_aas\n",
    "#44) mann_whitney\n",
    "#45) t_test\n",
    "#46) extract_nested_values\n",
    "#47) PCA_feature_importance\n",
    "#48) convert_tr_ids\n",
    "#49) calc_GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d2636-61ca-4e88-b353-bff865c9372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSA functions:\n",
    "#1) get distance matrix of pairwise global alignments, count differences between two aligned strings \n",
    "#def count_diffs(a1, a2):\n",
    "#when a1[i]!=a2[i]\n",
    "# diff_score +=1\n",
    "#def alignment_diffs(s1, s2, gap, match_fn):\n",
    "#call global_alignment, then count_diffs\n",
    "#def guidetree_matrix(sequences, gap, match_fn):\n",
    "#initialise matrix\n",
    "#for i in range(len(sequences[:-1])):\n",
    "# for j in range(len(sequences[1:])):\n",
    "#matrix[i,j] and matrix[j,i] == alignment_diffs(sequences[i],sequences[j])\n",
    "\n",
    "#2) Guide tree traversal:\n",
    "#def find_nearest(mat):\n",
    "# find smallest distance in the matrix\n",
    "# set min_val = np.max(mat)+1\n",
    "# for row[0:-1], from col starting in row+1:\n",
    "# if mat[row, col] <min_val, set min_row and min_col and min_val\n",
    "# return values of min_row, min_col, and min_val\n",
    "#def UPGMA_step(mat):\n",
    "#use find_nearest() to merge elements i1 and i2 (nearest elems)\n",
    "#no.copy(mat)\n",
    "#set row i1 to average of i1 and i2 in origincal matrix\n",
    "#same for column i1\n",
    "#set [i1,i1] =0\n",
    "#delete row and column i2 with np.delete(new_mat, i2, axis=0) (and axis=1)\n",
    "#return i1, i2, and new matrix\n",
    "\n",
    "#3) compute consensus\n",
    "#def column_consensus(mat, col):\n",
    "#create a list with residue in position col of all sequences in msa\n",
    "# compute mode of this list (scipy.stats import mode)\n",
    "# this function returns most common residue and count\n",
    "#if count[0]<one half of sequences or mode[0] is negative, return \"X\"\n",
    "#else return mode[0]\n",
    "#def create_consensus(msa):\n",
    "#start with an empty string and add consensuss for each column\n",
    "\n",
    "#4) merge MSA\n",
    "#align consensi\n",
    "#pad with gaps in consensus alignment\n",
    "#join two MSAs (add lists) and recompute consensus\n",
    "#def pad_msa(msa, alignment):\n",
    "#create new msa=[\"\"]*len(msa) and current_pos = 0\n",
    "# for each index in alignment:\n",
    "#if \"-\", add \"-\" to each string in new_msa\n",
    "#else, add character in current_pos of each sequence\n",
    "#increment current_pos\n",
    "#def combine_msa(old_list, i1, i2, new):\n",
    "#create empty list\n",
    "#for eac index in old_list, if it is i1 append the new element\n",
    "#if it is !=i2, append that element from old list (discards i2)\n",
    "\n",
    "#5) follow the guidetree\n",
    "#def clustal_step( matrix, msas, consensus, names, gap, match_fn):\n",
    "#i1,i2,new_mat = UPGMA_step(matrix)\n",
    "#a1,a2,score = global_alignment(consensus[i1], consensus[i2], gap, match_fn)\n",
    "#new_msa= pad_msa(msas[i1],a1)+pad_msa(msas[i2],a2)\n",
    "#new_msas=combine_elements(msas, i1,i2,new_msa)\n",
    "#new_consensus=combine_elements(consensus,i1,i2,create_consensus(new_msa))\n",
    "#new_names = combine_elements(names,i1,i2,names[i1]+names[i2])\n",
    "#return new_mat,new_msas, new_consensus, new_names\n",
    "#\n",
    "#def do_clustal(sequences, names, gap, match_fn):\n",
    "#get matrix from guidetree_matrix(sequences,gap,match_fn)\n",
    "#while more than one element (in msas, names of consensus):\n",
    "#mat,msas,consensus,name_groups = clustal_step(...)\n",
    "#return msas[0],names[0]\n",
    "\n",
    "#6)phylogenetic tree\n",
    "#def differences_matrix(msa):\n",
    "#create a matrix, then for each pair of aligned sequences in MSA\n",
    "#use count_diffs[msa[i],msa[j]) to fill mat[i,j] and mat[j,i]\n",
    "#def newick_step(matrix, newick)\n",
    "#newick is a list of tuples (name,0)\n",
    "#i1,i2,new_mat = UPGMA_step(matrix)\n",
    "#branch = matrix[i1,i2]/2\n",
    "#n1,b1 = newick[i1]\n",
    "#n2,b2 = newick[i2]\n",
    "#new_newick_entry = (f\"({n1}:{branch-b1},{n2}:{branch-b2}\", branch)\n",
    "#new_nweick = combine_elements(newick,i1,i2,new_newick_entry)\n",
    "#return new_mat, new_newick\n",
    "#def create_newick(msa, names):\n",
    "#mat=differences_matrix(msa)\n",
    "#newick=[]\n",
    "#for n in names:\n",
    "#  newick.append((n,0))\n",
    "#while len(newick)>1:\n",
    "#  mat,newick = newick_step(mat,newick)\n",
    "#return newick[0][0]+\";\"\n",
    "\n",
    "#use tree visualiser to visualise newick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040e9ab-cdf6-43ab-853a-a0472a19c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global alignment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e15ff9a-9db9-4fc5-a90b-538563ab5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_alignment(s1,s2,gap,match_fn):\n",
    "    scores,moves=global_alignment_matrix(s1,s2,gap,match_fn)\n",
    "    a1,a2=traceback(s1,s2,moves)\n",
    "    return a1,a2,scores[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eaf80cb-36f3-4faa-a579-a3450e75860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceback(s1,s2,moves):\n",
    "    align1=\"\"\n",
    "    align2=\"\"\n",
    "    r=moves.shape[0]-1\n",
    "    c=moves.shape[1]-1\n",
    "    while r>0 or c>0:\n",
    "        if moves[r,c] ==0:\n",
    "            align1=s1[r-1]+align1\n",
    "            align2=\"-\"+align2\n",
    "            r-=1\n",
    "        elif moves[r,c] ==1:\n",
    "            align1=\"-\"+align1\n",
    "            align2=s2[c-1]+align2\n",
    "            c-=1\n",
    "        else:\n",
    "            align1=s1[r-1]+align1\n",
    "            align2=s2[c-1]+align2\n",
    "            r-=1\n",
    "            c-=1\n",
    "    return align1, align2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b90e5cb-5024-4d0e-957d-2c48d7343684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_fn(r1,r2):\n",
    "    if r1==r2:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c9101f-783e-4844-a58b-34c245e2e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_alignment_matrix(s1, s2, gap, match_fn):\n",
    "#Needleman-Wunsch?\n",
    "    import numpy as np\n",
    "    scores = np.zeros((len(s1)+1,len(s2)+1))\n",
    "    moves = np.zeros((len(s1)+1,len(s2)+1))\n",
    "    moves[0,:] = 1\n",
    "    for ix in range(scores.shape[1]):\n",
    "        scores[0,ix]=gap*ix\n",
    "    for ix in range(scores.shape[0]):\n",
    "        scores[ix,0]=gap*ix\n",
    "    for line in range(1,len(s1)+1):\n",
    "        for col in range(1,len(s2)+1):\n",
    "            down=scores[line-1,col]+gap\n",
    "            right=scores[line,col-1]+gap\n",
    "            match=scores[line-1,col-1]+match_fn(s1[line-1],s2[col-1])\n",
    "            scores[line,col] = max((down,right,match))\n",
    "            moves[line,col] = np.argmax((down,right,match))\n",
    "    return scores,moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e333f6a4-82cd-4380-ad32-af9d690db9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def substitution_matrix(seq1, seq2, match=2, mismatch=-1, gapopen=-2, gapextend=-1):\n",
    "#     s1 = list(max(seq1,seq2))\n",
    "#     s2 = list(min(seq1,seq2))\n",
    "#     l1 = len(s1)\n",
    "#     l2 = len(s2)\n",
    "    \n",
    "#     M = [[0]*(l1+1) for i in range(l2+1)]\n",
    "    \n",
    "#     for i in range(1,l2+1):\n",
    "#         for j in range(1,l1+1):\n",
    "#             if s1[j-1]==s2[i-1]:\n",
    "#                 M[i][j]=1\n",
    "#             for row in M:\n",
    "#                 print(row)\n",
    "#             print(\"+++++++++++++++++++++++++++++++++=\")\n",
    "            \n",
    "#     S = [[0]*(l1+1) for i in range(l2+1)]\n",
    "    \n",
    "#     for i in range(1,l2+1):\n",
    "#         for j in range(1,l1+1):\n",
    "#             if M[i][j] == 1:\n",
    "#                 if M[i-1][j-1] == 1:\n",
    "#                     S[i][j] = S[i-1][j-1]+3\n",
    "#                 elif M[i-1][j] == 1:\n",
    "#                     S[i][j] = S[i-1][j]-2\n",
    "#                 elif M[i][j-1] == 1:\n",
    "#                     S[i][j] = S[i][j-1]-2\n",
    "#                 elif S[]:\n",
    "#                     S[i][j]\n",
    "#                 elif\n",
    "\n",
    "#for (i, j) need to check (i-1, j) and (i, j-1) for gap open/extension i.e. take their score and -2 if gap open or -1 if extend or -1 if mismatch\n",
    "#, and (i-1, j-1) for match+2 or mismatch -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0244984-4f76-4f49-b277-b171fa9a1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_kmers1(seq, klen, mode=\"DNA\",):\n",
    "    kmer_dict = {}\n",
    "    if mode == \"DNA\":\n",
    "        DNA = \"ACTG\"\n",
    "    elif mode == \"protein\":\n",
    "        DNA = \"ACDEFGHIKLMNPQRSTVWY*\"\n",
    "    num_kmers = len(seq) - klen +1\n",
    "    for i in range(num_kmers):\n",
    "        kmer=seq[i:i+klen]\n",
    "        if kmer in kmer_dict:\n",
    "            kmer_dict[kmer] +=1\n",
    "        else:\n",
    "            kmer_dict[kmer] =1\n",
    "\n",
    "    #non_zero = {k:v for k, v in kmer_dict.items() if v != 0}\n",
    "    return kmer_dict #non_zero\n",
    "\n",
    "#non_zero = {k:v for k, v in diction.items() if v != 0}\n",
    "#non_zero = dict(filter(lambda kv: kv[1] != 0 , diction.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc3310-6c74-4cd3-a677-ceeed28acfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# s1=\"CTGTCGTGAATGTACGATAAAAAAAAAAAAAAAACGTAGCTGATGCGGTTTTCTCAGCTACGTTAGTCGTGCATCGAGCATGATCTAATGGGCCCCGGGGCTTCTATTAATCACTATCTTCATTTTGAATTCTTCCTTACACCCCCCTATGATGTATTTCATTCGTAGGAGGGAGAGTCTATAGTATATTTCATTTTTCTAAATCGAAAAAGGTCTCCAACCCAATCGGCCACTACCTAGTCCACCATACTGCACACTCATACTACATCGTACGGGGGGGGTTTTTTTTGCTAAAAAAAAAACCCCCCCCCCCCCCCATCGTACGTACTG\"\n",
    "# s2=\"TCTTTTCCTTTTTCAGGGCTAGCTAGCATCTTTGAGGTCGATCGTAGCAGCGGGAGAAAAATTCCCCCCCCGAAAAAAAAAAAGCCCCCCCCCCCTAGTCAGCTGACTGGCAGGAGGGTTCTCTCCTGAGGAAATTGAGGGAGAGGAGAGGAGAGAGGGGAGTATACTCTCGTGTGTCCCCCCCCCCTAGCGGGCTAGGCGGTTCTCTTTCTCTGGGGATCGATCGGCATGCTACGTGCATCGATCAGACTTGGGCTCTGCAACATAGATTGACACAAAGTCACGCGCTTTTAGTTACCCCGATCGATCGCCCCCGCCGGTACGTGAAAAAAAAAAAAGC\"\n",
    "# q=list(count_kmers1(s1,5).keys())\n",
    "# w=list(count_kmers1(s2,5).keys())\n",
    "# st1 = time.time()\n",
    "# if set(q).isdisjoint(set(w))==0:\n",
    "#     print(\"q\")\n",
    "# st2 = time.time()\n",
    "# print(\"set\", st2-st1)\n",
    "\n",
    "# st1 = time.time()\n",
    "# if any(deca in w for deca in q):\n",
    "#     print(\"w\")\n",
    "# st2 = time.time()\n",
    "# print(\"comp\", st2-st1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d912d-1922-4063-9aee-b1ea883a12a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc2f00-11ea-4d74-8cfc-dbe92dfec18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapeptide_filter_query(stats_df, df=False, mode=\"protein\", mer=10):\n",
    "    if mode==\"protein\":\n",
    "        seq_col = \"seq\"\n",
    "        kmer_col = \"decapeptides\"\n",
    "    else:\n",
    "        seq_col = \"seq\"\n",
    "        kmer_col = \"decamers\"\n",
    "        \n",
    "    stats_df = stats_df.sort_values(by=seq_col, ascending=False).reset_index(drop=True)\n",
    "    stats_df[kmer_col] = \"\"\n",
    "    stats_df_work = stats_df[[\"id\",seq_col,kmer_col,]]\n",
    "    rep_groups_90 = {}\n",
    "    rep_groups_95 = {}\n",
    "    stats_df_work[kmer_col][0] = list(count_kmers1(stats_df_work[seq_col][0], klen=mer, mode=mode).keys())\n",
    "    rep_set = [pd.DataFrame(stats_df_work.iloc[0]).T]\n",
    "    \n",
    "    #while i < stats_df_work.shape[0]:\n",
    "    for i in range(1,stats_df_work.shape[0]):\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "        for j in range(len(rep_set)):\n",
    "            ident_over_90, ident_over_95, rep_kmers, query_kmers = decapeptide_filter(rep_set[j][seq_col][j], rep_set[j][\"id\"][j], \n",
    "                                                                                      stats_df_work[seq_col][i], stats_df_work[\"id\"][i],rep_kmers= rep_set[j][kmer_col][j],mode=mode, w=mer)\n",
    "            if ident_over_90==True:\n",
    "                if rep_set[j][\"id\"][j] not in rep_groups_90:\n",
    "                    rep_groups_90[rep_set[j][\"id\"][j]] = [stats_df_work[\"id\"][i]]\n",
    "                    break\n",
    "                else:\n",
    "                    rep_groups_90[rep_set[j][\"id\"][j]] += [stats_df_work[\"id\"][i]]\n",
    "                    break\n",
    "            elif ident_over_95==True:\n",
    "                if rep_set[j][\"id\"][j] not in rep_groups_90:\n",
    "                    rep_groups_95[rep_set[j][\"id\"][j]] = [stats_df_work[\"id\"][i]]\n",
    "                    break\n",
    "                else:\n",
    "                    rep_groups_95[rep_set[j][\"id\"][j]] += [stats_df_work[\"id\"][i]]\n",
    "                    break\n",
    "        else:\n",
    "            app = pd.DataFrame(stats_df_work.iloc[i]).T\n",
    "            app[kmer_col] = [query_kmers]\n",
    "            app.index = [len(rep_set)]\n",
    "            rep_set.append(app)\n",
    "\n",
    "    if df:\n",
    "        rep_g90 = pd.DataFrame()\n",
    "        rep_g90[\"leader\"] = rep_groups_90.keys()\n",
    "        rep_g90[\"members\"] = rep_groups_90.values()\n",
    "        rep_groups_90 = rep_g90\n",
    "        #contains group leaders and their corresponding group members\n",
    "\n",
    "        rep_g95 = pd.DataFrame()\n",
    "        rep_g95[\"leader\"] = rep_groups_95.keys()\n",
    "        rep_g95[\"members\"] = rep_groups_95.values()\n",
    "        rep_groups_95 = rep_g95\n",
    "        #contains group leaders and their corresponding group members\n",
    "        \n",
    "        rep_df = pd.concat(rep_set)\n",
    "        rep_set=rep_df\n",
    "        #contains group leaders and singletons, and their decamer breakdown\n",
    "    \n",
    "    return rep_groups_90, rep_groups_95, rep_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc22e90-f923-4a66-992a-8ad107f6da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapeptide_filter(rep_seq, rid, query, qid, w=10, rep_kmers=False, mode=\"protein\",):\n",
    "    #90% threshold\n",
    "    seq_len = min(len(query),len(rep_seq))\n",
    "    \n",
    "    ident_over_90=False\n",
    "    ident_over_95=False\n",
    "    \n",
    "    query_kmers = list(count_kmers1(query, klen=w, mode=mode).keys())\n",
    "    if rep_kmers==False:\n",
    "        rep_kmers = list(count_kmers1(rep_seq, klen=w, mode=mode).keys())\n",
    "    if any(deca in rep_kmers for deca in query_kmers):\n",
    "        if seq_len<20:\n",
    "            ident_over_95=True\n",
    "        else:\n",
    "            ident_over_90=True\n",
    "    \n",
    "    return ident_over_90, ident_over_95, rep_kmers, query_kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "200eb517-fceb-4c22-a582-832f4a7a82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ktuple_filter(seq1, seq2, w=10, k=10, thresh_pident=90):\n",
    "    thresh_match = int(k/100*thresh_pident)+1\n",
    "    seq_len = min(len(seq1),len(seq2))\n",
    "    mismatches_allowed = 100-(int(seq_len/100*thresh_pident)+1)\n",
    "    a = int(seq_len/w)\n",
    "    b = seq_len%w\n",
    "    num_ktuples = a-mismatches_allowed\n",
    "    min_ktuples = w*num_ktuples-(w-1)\n",
    "    thresh_match = seq_len/100*thresh_pident\n",
    "    return mismatches_allowed, a, b, num_ktuples, min_ktuples\n",
    "\n",
    "# import scipy.special\n",
    "# b=0\n",
    "# L=100\n",
    "# w=5\n",
    "# k=5\n",
    "# p=k/w-1/w+b/w*(w-k+1)/L\n",
    "# wk_binom = scipy.special.binom(10,10)\n",
    "# T=(L-w+1)*wk_binom+w-k # number of ktuples covering the query\n",
    "# m=n*k*wk_binom # number of mismatching ktuples\n",
    "# M = w-k+(L-w+1-(1-p)*L*k)*wk_binom # number of matching ktuples at given k,w,p,and L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b129479-3fa4-4f27-b049-c6385365a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df_by_part2(df_part_dir):\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    i=0\n",
    "    pathlist = glob.glob(os.path.join(df_part_dir,\"*\"))\n",
    "    for infile in sorted(pathlist):\n",
    "        if i==0:\n",
    "            fin=pd.read_csv(infile, sep=\"\\t\")\n",
    "            icols = fin.select_dtypes(\"integer\").columns\n",
    "            fin[icols] = fin[icols].apply(lambda x: pd.to_numeric(x, downcast=\"integer\"))\n",
    "            i+=1\n",
    "        else:\n",
    "            p=pd.read_csv(infile, sep=\"\\t\")\n",
    "            icols = p.select_dtypes(\"integer\").columns\n",
    "            p[icols] = p[icols].apply(lambda x: pd.to_numeric(x, downcast=\"integer\"))\n",
    "            fin=pd.concat([fin,p],axis=0,ignore_index=True)\n",
    "            fin = fin.fillna(0)\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ba7876b-78a8-401c-bcb8-2ab926210cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df_by_part1(df_part_dir):\n",
    "    from pathlib import Path\n",
    "    pathlist = Path(df_part_dir).glob(\"**/*\")\n",
    "    for path in pathlist:\n",
    "        path_in_str=str(path)\n",
    "        print(path_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ede4249-de73-49b2-8f9e-c8ecb866b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df_by_part(df_part_dir):\n",
    "    import os\n",
    "    dir = os.fsencode(df_part_dir)\n",
    "    for file in os.listdir(dir):\n",
    "        fname=os.fsdecode(file)\n",
    "        print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73ed4df5-48b4-4106-b1de-469b48e44c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_coords(seq, subseq):\n",
    "    # \"TATAGCATATC\" returns [1] [4]\n",
    "    count = start = 0\n",
    "    starts = []\n",
    "    while True:\n",
    "        start = seq.find(subseq, start) + 1\n",
    "        if start > 0:\n",
    "            starts.append(start)\n",
    "        else:\n",
    "            ends = [x+len(subseq)-1 for x in starts]\n",
    "            return starts, ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ccd2ca63-d0e5-4f86-bd6a-c0f7e01335ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_occurrences(seq, subseq):\n",
    "    count = start = 0\n",
    "    while True:\n",
    "        start = seq.find(subseq, start) + 1\n",
    "        if start > 0:\n",
    "            count += 1\n",
    "        else:\n",
    "            return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "98f7bcb6-085e-4ae0-8abb-bd67e5ac3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_polyA(seq, min_polyA = 15, rev_comp=False, get_seq=False, contains=True, polyA_list=[], polyA_pattern=[]):\n",
    "    #rev_comp - whether to search for polyA via reverse complementing the sequence - useless with kmer approach\n",
    "    #get_seq - return sequence\n",
    "    #contains - performs any() before doing regex search. more speed benefit when there are many negatives\n",
    "    #polyA_list - list of generated polyAs, directly used for contains segment, can be filled with anything random if contains is False, but cant be left empty if kmers used\n",
    "    #polyA_pattern - pattern containing all kmers generated by \"generate_impure_pA\" function\n",
    "    false_positive=True\n",
    "    seq_len = len(seq)\n",
    "    import re\n",
    "    polyA_re = r\"\\1\"*(min_polyA-2)\n",
    "    pattern = fr\"([A]+?){polyA_re}\\1+\"\n",
    "    pA = re.finditer(pattern,seq)\n",
    "    pattern2 = fr\"([T]+?){polyA_re}\\1+\"\n",
    "    pA2 = re.finditer(pattern2,seq)\n",
    "    starts = []\n",
    "    ends = []\n",
    "    pA_lens = []\n",
    "    rev_compd=False\n",
    "    non_consecutive=False\n",
    "    if pA:\n",
    "        for i in pA:\n",
    "            starts.append(i.start())\n",
    "            ends.append(i.end())\n",
    "            pA_lens.append(len(i[0]))\n",
    "    if pA2:\n",
    "        for i in pA2:\n",
    "            starts.append(i.start())\n",
    "            ends.append(i.end())\n",
    "            pA_lens.append(len(i[0]))\n",
    "    if len(starts) == 0:\n",
    "        if rev_comp == True:\n",
    "            polyA_re = r\"\\1\"*(min_polyA-2)\n",
    "            pattern = fr\"([A]+?){polyA_re}\\1+\"\n",
    "            seq = rev_comp(seq,mode=\"rev_comp\")[0]\n",
    "            pA = re.finditer(pattern,seq)\n",
    "            starts = []\n",
    "            ends\n",
    "            pA_lens = []\n",
    "            rev_compd = True\n",
    "            for i in pA:\n",
    "                starts.append(i.start())\n",
    "                ends.append(i.end())\n",
    "                pA_lens.append(len(i[0]))\n",
    "        if len(starts) == 0:\n",
    "            if len(polyA_list)>0:\n",
    "                if rev_comp == True:\n",
    "                    polyA_re = r\"\\1\"*(min_polyA-2)\n",
    "                    pattern = fr\"([A|T]+?){polyA_re}\\1+\"\n",
    "                    seq = rev_comp(seq,mode=\"rev_comp\")[0]\n",
    "                    pA = re.finditer(pattern,seq)\n",
    "                elif contains == True:\n",
    "                    if any(pAgen in seq for pAgen in polyA_list):\n",
    "                        pA = re.finditer(polyA_pattern, seq)\n",
    "                else:\n",
    "                    pattern = fr\"({polyA_pattern})\"\n",
    "                    pA = re.finditer(pattern,seq)\n",
    "            starts = []\n",
    "            ends = []\n",
    "            pA_lens = []\n",
    "            for i in pA:\n",
    "                starts.append(i.start())\n",
    "                ends.append(i.end())\n",
    "                pA_lens.append(len(i[0]))\n",
    "            non_consecutive = True\n",
    "            if len(starts)>0:\n",
    "                false_positive=True\n",
    "                if min(starts) <150:\n",
    "                    false_positive = False\n",
    "                    rev_compd=True\n",
    "                if max(ends)>seq_len-150:\n",
    "                    false_positive = False\n",
    "\n",
    "        else:\n",
    "            false_positive=True\n",
    "            if min(starts) <150:\n",
    "                false_positive = False\n",
    "                rev_compd=True\n",
    "            if max(ends)>seq_len-150:\n",
    "                false_positive = False\n",
    "\n",
    "    else:\n",
    "        false_positive=True\n",
    "        if min(starts) <150:\n",
    "            false_positive = False\n",
    "            rev_compd=True\n",
    "        if max(ends)>seq_len-150:\n",
    "            false_positive = False\n",
    "\n",
    "    if get_seq == False:\n",
    "        seq=\"\"\n",
    "        \n",
    "    return starts, ends, pA_lens, rev_compd, seq, non_consecutive, seq_len, false_positive\n",
    "    #starts - start coords\n",
    "    #ends - end coords\n",
    "    #pA_lens - pA lengths\n",
    "    #rev_compd - whether it was required to rev_comp the sequence to find the pA\n",
    "    #seq - sequence, will be rev_comped if it was to find pA\n",
    "    #non_consecutive - whether it was necessary to use pre-generated impure kmers to find the pA\n",
    "    #false_positive - whether the pA was found at one of the ends of the sequence or in the middle (false positie)\n",
    "#example:\n",
    "#pA_starts, pA_ends, pA_lens, pA_ev_compd, pA_seq, pA_non_consecutive, pA_seq_len, pA_false_positive = search_polyA(seq, contains=True, polyA_list=pA_gen, polyA_pattern=pA_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "63809157-5ee3-477c-95ad-f5eef4d03a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_impure_pA(chain=15, impure=1, pattern=True):\n",
    "    alphabet = [\"A\",\"C\",\"T\",\"G\"]\n",
    "    pure_a = [\"A\", \"T\"]\n",
    "    seq_list = []\n",
    "\n",
    "    if impure == 1:\n",
    "        for letter in pure_a:\n",
    "            seq_list.append(letter*chain)\n",
    "            alph = [x for x in alphabet if x not in [letter]]\n",
    "            for impurity in alph:\n",
    "                for i in range(chain):\n",
    "                    next_seq = list(letter*chain)\n",
    "                    next_seq[i] = impurity\n",
    "                    next_seq = \"\".join(next_seq)\n",
    "                    seq_list.append(next_seq)\n",
    "\n",
    "    if impure == 2:\n",
    "        for letter in pure_a:\n",
    "            seq_list.append(letter*chain)\n",
    "            alph = [x for x in alphabet if x not in [letter]]\n",
    "            for impurity in alph:\n",
    "                for i in range(chain):\n",
    "                    for j in range(i,chain):\n",
    "                        matrix = [0]*chain\n",
    "                        if i==j:\n",
    "                            z = i\n",
    "                        matrix[j]=1\n",
    "                        matrix[z]=1\n",
    "                        next_seq = list(map(lambda x: impurity if x != 0 else letter, matrix))\n",
    "                        next_seq = \"\".join(next_seq)\n",
    "                        seq_list.append(next_seq)\n",
    "\n",
    "    if pattern == True:\n",
    "        pattern = \"\"\n",
    "        for i in seq_list:\n",
    "            pattern += i\n",
    "            pattern += \"|\"\n",
    "        pattern = pattern[:-1]\n",
    "\n",
    "    return seq_list, pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea4b533-a56d-43a3-8bb8-d0ff764d7ef9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_tr_ORF_stats(seq_list, seq_ids, save_path=False, batch_size=10000,pA_chain_gen=[20,2], check_MREs=True, include_seq=False,tissue=False):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    tr_stats = []\n",
    "    i=0\n",
    "    j=0\n",
    "    z=0\n",
    "    pA_gen, pA_pattern = generate_impure_pA(chain=pA_chain_gen[0], impure=pA_chain_gen[1])\n",
    "    \n",
    "    for seq_idx in range(len(seq_list)):\n",
    "        j+=1\n",
    "        if j%batch_size == 0:\n",
    "            print(j)\n",
    "        if save_path!=False:\n",
    "            if j%batch_size == 0:\n",
    "                tr_stats = pd.concat(tr_stats, axis=0)\n",
    "                tr_stats = tr_stats.reset_index(drop=True)\n",
    "                tr_stats.to_csv(save_path+f\"_{j}\", index=False, sep=\"\\t\")\n",
    "                i=0\n",
    "        out = find_ORFs(seq_list[seq_idx], len_filter=26, stops=[\"TAA\",\"TGA\",\"TAG\"], to_translate=True)\n",
    "        if len(out[3])>0:\n",
    "            stats_df, cols = flnc_tr_seq_stats_no_pd(seq_list[seq_idx], seq_ids[seq_idx], ORF_coords=out[5],polyA_pattern=pA_pattern, polyA_list=pA_gen,check_MREs=check_MREs, all_ORFs=out[0], ORFs_out=out, include_seq=include_seq,tissue=tissue)\n",
    "            # stats_df = pd.DataFrame([[x for x in stats_df]])\n",
    "            # stats_df = pd.DataFrame([stats_df])\n",
    "            if i == 0:\n",
    "                tr_stats = []\n",
    "                tr_stats.append(stats_df)\n",
    "                # tr_stats = stats_df.copy()\n",
    "                i+=1\n",
    "            else:\n",
    "                tr_stats.append(stats_df)\n",
    "        else:\n",
    "            if z==0:\n",
    "                print(\"No ORFs found#\")\n",
    "                #stats_df = pd.DataFrame()\n",
    "                z+=1\n",
    "            else:\n",
    "                print(\"No ORFs found\")\n",
    "                #stats_df[\"num_ORFs_found\"] = 0\n",
    "                \n",
    "    if save_path!=False:\n",
    "        if j%batch_size == 0:\n",
    "            tr_stats = pd.concat(tr_stats, axis=0)\n",
    "            tr_stats = tr_stats.reset_index(drop=True)\n",
    "            tr_stats.to_csv(save_path+f\"_{j}\", index=False, sep=\"\\t\")\n",
    "            i=0\n",
    "    else:\n",
    "        tr_stats = pd.DataFrame(tr_stats)\n",
    "        tr_stats.columns = cols\n",
    "        # tr_stats = pd.concat(tr_stats,axis=0)\n",
    "        \n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    return tr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e298b17-e538-4164-8b8a-4c2830a28b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_frame(seq, subseq):\n",
    "    substring_starts = []\n",
    "    i=0\n",
    "    while i != -1:\n",
    "        pi = i\n",
    "        i = seq[i:].find(subseq)\n",
    "        if i != -1: \n",
    "            i = pi + i + 1\n",
    "            substring_starts.append(i)\n",
    "    frames=[]\n",
    "    if len(substring_starts) == 0:\n",
    "        print(\"No subseq match found\")\n",
    "    else:\n",
    "        for start in substring_starts:\n",
    "            f = start%3\n",
    "            if f == 0:\n",
    "                frame = 3\n",
    "            elif f == 1:\n",
    "                frame = 1\n",
    "            elif f == 2:\n",
    "                frame = 2\n",
    "            frames.append(frame)\n",
    "    return frames, substring_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "60e44ba4-2e44-40be-9c66-28318ac95591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_frame_by_coord(start_coords):\n",
    "    #zero indexed coords from re.finditer\n",
    "    frames=[]\n",
    "    if len(start_coords) == 0:\n",
    "        print(\"No coords present\")\n",
    "    else:\n",
    "        for start in start_coords:\n",
    "            f = start%3\n",
    "            if f == 0:\n",
    "                frame = 1\n",
    "            elif f == 1:\n",
    "                frame = 2\n",
    "            elif f == 2:\n",
    "                frame = 3\n",
    "            frames.append(frame)\n",
    "    return frames\n",
    "#can implement an option to extract subseqs if end or len is supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "id": "01a0c2dd-ba39-4a4f-a0d5-ac0b9db2a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flnc_tr_seq_stats_no_pd(seq, seq_id=\"placeholder\", ORF_coords=[], stops=[\"TAA\",\"TAG\",\"TGA\"], min_polyA=15, seq_store=False, polyA_list=[],polyA_pattern=[], ORFs_out=[], check_MREs=True, all_ORFs=[], include_seq=False, tissue=False):\n",
    "    seq_len = len(seq)\n",
    "    stop_num = 0\n",
    "    stop_count = {}\n",
    "    for codon in stops:\n",
    "        i = substring_occurrences(seq,codon)\n",
    "        stop_count[codon] = i #stop counts dict\n",
    "        stop_num += i #total stops\n",
    " \n",
    "    G = seq.count(\"G\")\n",
    "    C = seq.count(\"C\")\n",
    "    GC = 100/seq_len*(G+C) #GC\n",
    "\n",
    "    ATG_num = substring_occurrences(seq,\"ATG\") #starts\n",
    "    seq_start_ratio = ATG_num/seq_len #start ratio\n",
    "    seq_stop_ratio = stop_num/seq_len #stop ratio\n",
    "    \n",
    "    TGT_num = substring_occurrences(seq, \"TGT\") #cys1\n",
    "    TGC_num = substring_occurrences(seq, \"TGC\") #cys2\n",
    "    seq_TGT_ratio = TGT_num/seq_len #\n",
    "    seq_TGC_ratio = TGC_num/seq_len #\n",
    "    Cys_codon_ratio = seq_TGT_ratio+seq_TGC_ratio #\n",
    "\n",
    "    tissue_id=None\n",
    "#    if include_seq:\n",
    "#        stats_df[\"seq\"] = seq #\n",
    "    if tissue:\n",
    "        tissues = [\"crop\", \"cal_g\", \"neph\", \"nerve_c\", \"clitel\", \"bod_w\", \"phar\", \"gut\", \"gizz\", \"sem_v\"] \n",
    "        for t in tissues:\n",
    "            if t in seq_id:\n",
    "                tissue_id = t #tissue id\n",
    "                break\n",
    "            \n",
    "    #can be optimised by merging with ORF code\n",
    "    #can also be expanded by splitting by stop types \n",
    "    frames, locs = which_frame(seq, \"ATG\")\n",
    "    frame_starts = [0]*3\n",
    "    for frame in frames:\n",
    "        frame_starts[frame-1] += 1 # frame start counts by frame\n",
    "\n",
    "    frame_stops = [0]*3\n",
    "    for stop in stops:\n",
    "        frames, locs = which_frame(seq, stop)\n",
    "        for frame in frames:\n",
    "            frame_stops[frame-1] += 1 # frame stop counts by frame\n",
    "            \n",
    "    if len(ORF_coords)>0: # Largest_ORF_coords all_ORFs # All ORF coords\n",
    "        ORF_stop = seq[ORF_coords[1]-3:ORF_coords[1]] # ORF_stop_seq\n",
    "        ORF_frame = ORF_coords[0]%3\n",
    "        if ORF_frame == 0:\n",
    "            ORF_frame = 3\n",
    "        elif ORF_frame == 1:\n",
    "            ORF_frame = 1\n",
    "        elif ORF_frame == 2:\n",
    "            ORF_frame = 2 #largest ORF frame\n",
    "        \n",
    "        ORF_seq = seq[ORF_coords[0]-1:ORF_coords[1]]\n",
    "        all_ORF_coords = all_ORFs\n",
    "        p5_seq = seq[0:ORF_coords[0]]\n",
    "        p3_seq = seq[ORF_coords[1]-1:seq_len]\n",
    "        seq_list = [p5_seq, ORF_seq, p3_seq]\n",
    "        prot_seq = translate(ORF_seq) # ORF prot\n",
    "        all_prot_seq=[]\n",
    "        all_prot_cys = 0\n",
    "        for orf in all_ORF_coords:\n",
    "            p = translate(seq[orf[0]-1:orf[1]])\n",
    "            all_prot_seq.append(p)\n",
    "            all_prot_cys += p.count(\"C\")\n",
    "        prot_len = len(prot_seq) # ORF prot leb\n",
    "        Cys_num = prot_seq.count(\"C\") # cys num\n",
    "        if Cys_num>0:\n",
    "            cys_ratio = Cys_num/prot_len # prot cys \n",
    "        else:\n",
    "            cys_ratio = 0\n",
    "        \n",
    "        ORF_seq_len = len(ORF_seq)\n",
    "        p5_seq_len = len(p5_seq)\n",
    "        p3_seq_len = len(p3_seq)\n",
    "        region_lens = [p5_seq_len, ORF_seq_len, p3_seq_len] # region lens\n",
    "\n",
    "        ORF_ATG_num = substring_occurrences(ORF_seq,\"ATG\") # ORF_ATG_total\n",
    "        ORF_TGT_num = substring_occurrences(ORF_seq,\"TGT\") # ORF_TGT_total\n",
    "        ORF_TGC_num = substring_occurrences(ORF_seq,\"TGC\") # ORF_TGC_total\n",
    "        ORF_TGT_ratio = ORF_TGT_num/ORF_seq_len # ORF_TGT_ratio\n",
    "        ORF_TGC_ratio = ORF_TGC_num/ORF_seq_len # ORF_TGC_ratio\n",
    "        ORF_Cys_ratio = ORF_TGT_ratio+ORF_TGC_ratio # ORF_Cys_ratio\n",
    "        \n",
    "        p5_ATG_num = substring_occurrences(p5_seq,\"ATG\") \n",
    "        p3_ATG_num = substring_occurrences(p3_seq,\"ATG\") \n",
    "        ATG_list = [p5_ATG_num, ORF_ATG_num, p3_ATG_num] # region_start_nums\n",
    "        \n",
    "        regions = [\"p5\", \"ORF\", \"p3\"]\n",
    "        region_stops = {}\n",
    "        region_stop_ratios = []\n",
    "        region_start_ratios = []\n",
    "        for region in range(len(seq_list)):\n",
    "            r_stop_counts = {}\n",
    "            r_stop_num = 0\n",
    "            for codon in stops:\n",
    "                i = substring_occurrences(seq_list[region],codon)\n",
    "                r_stop_counts[codon] = i\n",
    "                r_stop_num += i\n",
    "            r_stop_counts[\"total\"] = r_stop_num\n",
    "            region_stops[regions[region]] = r_stop_counts #region_stop_stats\n",
    "\n",
    "            region_stop_ratios.append(region_stops[regions[region]][\"total\"]/region_lens[region]) #region_stop_ratios\n",
    "            region_start_ratios.append(ATG_list[region]/region_lens[region]) #region_start_ratios\n",
    "    \n",
    "    pA_Starts, pA_Ends, pA_Lengths, Rev_Compd, Seq, Non_Consecutive, Seq_Len, False_Positive = search_polyA(seq, polyA_list=polyA_list, polyA_pattern=polyA_pattern)\n",
    "    \n",
    "    spans = []\n",
    "    pA_lens = []\n",
    "    \n",
    "    for i in range(len(pA_Starts)):\n",
    "        spans.append([pA_Starts[i],pA_Ends[i]])\n",
    "        pA_lens.append(pA_Lengths[i])\n",
    "\n",
    "    if len(spans) == 0:\n",
    "        polyA_span=None # polyA_span\n",
    "        polyA_len=None # polyA_len\n",
    "        pA_rev_compd=Rev_Compd # pA_rev_compd\n",
    "        pA_non_consecutive=Non_Consecutive # pA_non_consecutive\n",
    "        pA_false_positive = False_Positive # pA_false_positive\n",
    "    else:\n",
    "        polyA_span=spans\n",
    "        polyA_len=pA_lens\n",
    "        pA_rev_compd=Rev_Compd\n",
    "        pA_non_consecutive=Non_Consecutive\n",
    "        pA_false_positive = False_Positive\n",
    "    pA_sig_span = []\n",
    "    \n",
    "    if len(pA_lens)>0:\n",
    "        pA_start = sorted([x for i in spans for x in i])[-2]\n",
    "        pA_sig_region = seq[pA_start-100:pA_start]\n",
    "        pA_sig_pattern = \"AATAAA\"\n",
    "\n",
    "        pA_sig_span = []\n",
    "        q=0\n",
    "        while q != -1:\n",
    "            pq = q\n",
    "            q = pA_sig_region[q:].find(pA_sig_pattern)\n",
    "            if q != -1: \n",
    "                q = pq + q + 1\n",
    "                g = q+pA_start-100\n",
    "                b = g+6\n",
    "                pA_sig_span.append([g,b]) #polyA_signal_span\n",
    "        \n",
    "    if check_MREs==True:\n",
    "        MREs_list = [\"TGCGCAC\",\"TGCGCGC\",\"TGCGCCC\",\"TGCGCTC\",\"TGCACAC\",\"TGCACGC\",\"TGCACCC\",\"TGCACTC\",\"GAGTGCA\",\"GGGTGCA\",\"GCGTGCA\",\"GTGTGCA\",\"GAGCGCA\",\"GGGCGCA\",\"GCGCGCA\",\"GTGCGCA\"]\n",
    "        MRE_starts = []\n",
    "        MRE_types = []\n",
    "        MRE_frames = []\n",
    "        for MRE in MREs_list:\n",
    "            q=0\n",
    "            while q != -1:\n",
    "                pq = q\n",
    "                q = seq[q:].find(MRE)\n",
    "                if q != -1: \n",
    "                    q = pq + q + 1\n",
    "                    MRE_starts.append(q) #MRE_starts\n",
    "                    MRE_types.append(MRE) #MRE_types\n",
    "                    MRE_frames.append(which_frame_by_coord([q])) #MRE_frames\n",
    "\n",
    "        MRE_number = len(MRE_starts) #MRE_number\n",
    "        MREs = list(zip(MRE_starts,MRE_types)) #MREs\n",
    "    \n",
    "    num_ORFs_found = ORFs_out[6]\n",
    "    lens_above_filter = ORFs_out[3]\n",
    "    prots =ORFs_out[4]\n",
    "    stops_above_filter = ORFs_out[2]\n",
    "    \n",
    "    cols = [\"id\", \"tissue\", \"seq\", \"seq_len\", \"GC\", \"stop_counts\", \"total_stops\", \"ATG_num\", \"seq_start_ratio\", \"seq_stop_ratio\", \"TGT_num\", \"TGC_num\", \"seq_TGT_ratio\",\"seq_TCT_ratio\", \"cys_codon_ratio\", \"starts_per_frame\", \"stops_per_frame\", \"ORF_coords\", \"all_ORF_coords\", \"num_ORFs_found\", \"ORF_lengths\", \"all_prots\", \"ORF_stop\", \"all_ORF_stops\", \"ORF_frame\", \"prot_seq\", \"prot_len\", \"cys_num\", \"cys_ratio\", \"region_lengths\", \"ORF_ATG_num\", \"ORF_TGT_num\", \"ORF_TGC_num\", \"ORF_TGT_ratio\", \"ORF_TGC_ratio\", \"ORF_cys_ratio\", \"region_ATGs\", \"region_stops\", \"region_stop_ratios\", \"region_start_ratios\", \"polyA_span\", \"polyA_len\", \"pA_rev_compd\", \"pA_non_consecutive\", \"pA_false_positive\", \"pA_sig_span\", \"MRE_starts\", \"MRE_types\", \"MRE_frames\", \"MRE_number\", \"MREs\"]\n",
    "    out = [ seq_id, tissue_id, seq, seq_len, GC, stop_count, stop_num, ATG_num, seq_start_ratio, seq_stop_ratio, TGT_num, TGC_num, seq_TGT_ratio, seq_TGC_ratio, Cys_codon_ratio, frame_starts, frame_stops, ORF_coords, all_ORF_coords, num_ORFs_found, lens_above_filter, prots, ORF_stop, stops_above_filter, ORF_frame, prot_seq, prot_len, Cys_num, cys_ratio, region_lens, ORF_ATG_num, ORF_TGT_num, ORF_TGC_num, ORF_TGT_ratio, ORF_TGC_ratio, ORF_Cys_ratio, ATG_list, region_stops, region_stop_ratios, region_start_ratios, polyA_span, polyA_len, pA_rev_compd, pA_non_consecutive, pA_false_positive, pA_sig_span, MRE_starts, MRE_types, MRE_frames, MRE_number, MREs]\n",
    "    return out, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "id": "980ef819-d86e-4244-b5c2-94de8abfc133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 1708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "40676c34-4739-41f0-8460-1aa6193a843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flnc_tr_seq_stats(seq, seq_id=\"placeholder\", ORF_coords=[], stops=[\"TAA\",\"TAG\",\"TGA\"], min_polyA=15, seq_store=False, polyA_list=[],polyA_pattern=[], check_MREs=True, all_ORFs=[], include_seq=False, tissue=False):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    stats_df = pd.DataFrame([1])\n",
    "    seq_len = len(seq)\n",
    "    stop_num = 0\n",
    "    stop_count = {}\n",
    "    for codon in stops:\n",
    "        i = substring_occurrences(seq,codon)\n",
    "        stop_count[codon] = i\n",
    "        stop_num += i\n",
    "        stats_df[f\"{codon}_total\"] = i\n",
    "\n",
    "    G = seq.count(\"G\")\n",
    "    C = seq.count(\"C\")\n",
    "    GC = 100/seq_len*(G+C)\n",
    "    stats_df[\"GC\"] = GC\n",
    "\n",
    "    ATG_num = substring_occurrences(seq,\"ATG\")\n",
    "    seq_start_ratio = ATG_num/seq_len\n",
    "    seq_stop_ratio = stop_num/seq_len\n",
    "    \n",
    "    TGT_num = substring_occurrences(seq, \"TGT\")\n",
    "    TGC_num = substring_occurrences(seq, \"TGC\")\n",
    "    seq_TGT_ratio = TGT_num/seq_len\n",
    "    seq_TGC_ratio = TGC_num/seq_len\n",
    "    Cys_codon_ratio = seq_TGT_ratio+seq_TGC_ratio\n",
    "\n",
    "    if seq_store == \"all\":\n",
    "        stats_df[\"seq\"] = seq\n",
    "\n",
    "    stats_df[\"id\"] = seq_id\n",
    "    stats_df[\"seq_len\"] = seq_len\n",
    "    stats_df[\"ATG_total\"] = ATG_num\n",
    "    stats_df[\"seq_start_ratio\"] = seq_start_ratio\n",
    "    stats_df[\"seq_stop_ratio\"] = seq_stop_ratio   \n",
    "    stats_df[\"TGT_total\"] = TGT_num\n",
    "    stats_df[\"TGC_total\"] = TGC_num\n",
    "    stats_df[\"seq_TGT_ratio\"] = seq_TGT_ratio\n",
    "    stats_df[\"seq_TGC_ratio\"] = seq_TGC_ratio  \n",
    "    stats_df[\"Cys_codon_ratio\"] = Cys_codon_ratio\n",
    "    \n",
    "    if include_seq:\n",
    "        stats_df[\"seq\"] = seq\n",
    "        if tissue:\n",
    "            stats_df[\"tissue\"] = \"\"\n",
    "            tissue_regex = r'(crop|cal_g|neph|nerve_c|clitel|bod_w|phar|gut|gizz|sem_v)'\n",
    "            stats_df[\"tissue\"] = stats_df[\"id\"].str.extract(tissue_regex, expand=False)\n",
    "\n",
    "    #can be optimised by merging with ORF code\n",
    "    #can also be expanded by splitting by stop types \n",
    "    frames, locs = which_frame(seq, \"ATG\")\n",
    "    frame_starts = [0]*3\n",
    "    for frame in frames:\n",
    "        frame_starts[frame-1] += 1\n",
    "        \n",
    "    frame_stops = [0]*3\n",
    "    for stop in stops:\n",
    "        frames, locs = which_frame(seq, stop)\n",
    "        for frame in frames:\n",
    "            frame_stops[frame-1] += 1\n",
    "            \n",
    "    stats_df[\"start_counts_by_frame\"] = [frame_starts]\n",
    "    stats_df[\"stop_counts_by_frame\"] = [frame_stops]\n",
    "    \n",
    "    if len(ORF_coords)>0:\n",
    "        stats_df[\"ORF_coords\"] = [ORF_coords]\n",
    "        stats_df[\"ORF_stop\"] = seq[ORF_coords[1]-3:ORF_coords[1]]\n",
    "        stats_df[\"all_ORF_coords\"] = [all_ORFs]\n",
    "        ORF_frame = ORF_coords[0]%3\n",
    "        if ORF_frame == 0:\n",
    "            ORF_frame = 3\n",
    "        elif ORF_frame == 1:\n",
    "            ORF_frame = 1\n",
    "        elif ORF_frame == 2:\n",
    "            ORF_frame = 2\n",
    "        stats_df[\"ORF_frame\"] = ORF_frame\n",
    "        \n",
    "        ORF_seq = seq[ORF_coords[0]-1:ORF_coords[1]]\n",
    "        p5_seq = seq[0:ORF_coords[0]]\n",
    "        p3_seq = seq[ORF_coords[1]-1:seq_len]\n",
    "        seq_list = [p5_seq, ORF_seq, p3_seq]\n",
    "        if seq_store == \"all\" or seq_store==\"regions\":\n",
    "            stats_df[\"region_seqs\"] = [seq_list]\n",
    "        prot_seq = translate(ORF_seq)\n",
    "        stats_df[\"ORF_prot\"] = prot_seq\n",
    "        prot_len = len(prot_seq)\n",
    "        stats_df[\"ORF_prot_len\"] = prot_len\n",
    "        Cys_num = prot_seq.count(\"C\")\n",
    "        stats_df[\"cys_num\"] = Cys_num\n",
    "        if Cys_num>0:\n",
    "            cys_ratio = Cys_num/prot_len\n",
    "            stats_df[\"cys_ratio\"] = cys_ratio\n",
    "        else:\n",
    "            stats_df[\"cys_ratio\"] = 0\n",
    "\n",
    "        ORF_seq_len = len(ORF_seq)\n",
    "        p5_seq_len = len(p5_seq)\n",
    "        p3_seq_len = len(p3_seq)\n",
    "        len_list = [p5_seq_len, ORF_seq_len, p3_seq_len]\n",
    "        stats_df[\"region_lens\"] = [len_list]\n",
    "\n",
    "        ORF_ATG_num = substring_occurrences(ORF_seq,\"ATG\")\n",
    "        ORF_TGT_num = substring_occurrences(ORF_seq,\"TGT\")\n",
    "        ORF_TGC_num = substring_occurrences(ORF_seq,\"TGC\")\n",
    "        ORF_TGT_ratio = ORF_TGT_num/ORF_seq_len\n",
    "        ORF_TGC_ratio = ORF_TGC_num/ORF_seq_len\n",
    "        ORF_Cys_ratio = ORF_TGT_ratio+ORF_TGC_ratio\n",
    "        \n",
    "        p5_ATG_num = substring_occurrences(p5_seq,\"ATG\")\n",
    "        p3_ATG_num = substring_occurrences(p3_seq,\"ATG\")\n",
    "        ATG_list = [p5_ATG_num, ORF_ATG_num, p3_ATG_num]\n",
    "        stats_df[\"region_start_nums\"] = [ATG_list]\n",
    "        stats_df[\"ORF_TGT_total\"] = ORF_TGT_num\n",
    "        stats_df[\"ORF_TGC_total\"] = ORF_TGC_num\n",
    "        stats_df[\"ORF_TGT_ratio\"] = ORF_TGT_ratio\n",
    "        stats_df[\"ORF_TGC_ratio\"] = ORF_TGC_ratio\n",
    "        stats_df[\"ORF_Cys_ratio\"] = ORF_Cys_ratio\n",
    "        \n",
    "        regions = [\"p5\", \"ORF\", \"p3\"]\n",
    "        region_stops = {}\n",
    "        \n",
    "        for region in range(len(seq_list)):\n",
    "            r_stop_counts = {}\n",
    "            r_stop_num = 0\n",
    "            for codon in stops:\n",
    "                i = substring_occurrences(seq_list[region],codon)\n",
    "                r_stop_counts[codon] = i\n",
    "                r_stop_num += i\n",
    "            r_stop_counts[\"total\"] = r_stop_num\n",
    "            region_stops[regions[region]] = r_stop_counts\n",
    "\n",
    "            stats_df[f\"{regions[region]}_stop_ratio\"] = region_stops[regions[region]][\"total\"]/len_list[region]\n",
    "            stats_df[f\"{regions[region]}_start_ratio\"] = ATG_list[region]/len_list[region]\n",
    "        stats_df[\"region_stop_stats\"] = [region_stops]\n",
    "\n",
    "    pA_Starts, pA_Ends, pA_Lengths, Rev_Compd, Seq, Non_Consecutive, Seq_Len, False_Positive = search_polyA(seq, polyA_list=polyA_list, polyA_pattern=polyA_pattern)\n",
    "    \n",
    "    spans = []\n",
    "    pA_lens = []\n",
    "    \n",
    "    for i in range(len(pA_Starts)):\n",
    "        spans.append([pA_Starts[i],pA_Ends[i]])\n",
    "        pA_lens.append(pA_Lengths[i])\n",
    "\n",
    "    if len(spans) == 0:\n",
    "        stats_df[\"polyA_span\"] = None\n",
    "        stats_df[\"polyA_len\"] = None\n",
    "        stats_df[\"pA_rev_compd\"] = Rev_Compd\n",
    "        stats_df[\"pA_non_consecutive\"] = Non_Consecutive\n",
    "        stats_df[\"pA_false_positive\"] = False_Positive\n",
    "    else:\n",
    "        stats_df[\"polyA_span\"] = [spans]\n",
    "        stats_df[\"polyA_len\"] = [pA_lens]\n",
    "        stats_df[\"pA_rev_compd\"] = Rev_Compd\n",
    "        stats_df[\"pA_non_consecutive\"] = Non_Consecutive\n",
    "        stats_df[\"pA_false_positive\"] = False_Positive\n",
    "\n",
    "    if len(pA_lens)>0:\n",
    "        pA_start = sorted([x for i in spans for x in i])[-2]\n",
    "        pA_sig_region = seq[pA_start-100:pA_start]\n",
    "        pA_sig_pattern = r\"((AATAAA)+?)\"\n",
    "        pA_sig = re.finditer(pA_sig_pattern, pA_sig_region)\n",
    "        pA_sig_span = []\n",
    "        for i in pA_sig:\n",
    "            pA_sig_coords = list(i.span())\n",
    "            pA_sig_left = pA_sig_coords[0]+pA_start-100\n",
    "            pA_sig_right = pA_sig_coords[1]+pA_start-100\n",
    "            pA_sig_span.append([pA_sig_left,pA_sig_right])\n",
    "        stats_df[\"polyA_signal_span\"] = [pA_sig_span]\n",
    "        \n",
    "    if check_MREs==True:\n",
    "        stats_df[\"MREs\"] = [[[i.start(),i.group(0)] for i in re.finditer(\"TGCGCAC|TGCGCGC|TGCGCCC|TGCGCTC|TGCACAC|TGCACGC|TGCACCC|TGCACTC|GAGTGCA|GGGTGCA|GCGTGCA|GTGTGCA|GAGCGCA|GGGCGCA|GCGCGCA|GTGCGCA\", seq)]]\n",
    "        stats_df[\"MRE_starts\"]=stats_df[\"MREs\"].apply(lambda x: [i[0] for i in x])\n",
    "        stats_df[\"MRE_types\"]=stats_df[\"MREs\"].apply(lambda x: [i[1] for i in x])\n",
    "        stats_df[\"MRE_frames\"]=stats_df[\"MRE_starts\"].apply(lambda x:which_frame_by_coord(x))\n",
    "        stats_df[\"MRE_starts_sorted\"]=stats_df[\"MRE_starts\"].apply(lambda x: sorted(x))\n",
    "        stats_df[\"MRE_number\"]=stats_df[\"MRE_starts\"].apply(lambda x: len(x))\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa0a6e-cc17-4c22-aadb-edf1be001036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prot_properties(seq, table=\"simple\"):\n",
    "    if table == \"simple\":\n",
    "        table = {\"R\":\"P\", \"H\":\"P\", \"K\":\"P\", \n",
    "            \"D\":\"P\", \"E\":\"P\", \n",
    "            \"S\":\"P\", \"T\":\"P\", \"N\":\"P\", \"Q\":\"P\", \n",
    "            \"A\":\"H\", \"V\":\"H\", \"I\":\"H\", \"L\":\"H\", \"M\":\"H\", \"F\":\"H\", \"Y\":\"H\", \"W\":\"H\",\n",
    "            \"C\": \"*\", \"G\":\"H\", \"P\":\"H\"}\n",
    "        \n",
    "    elif table ==\"charged\":\n",
    "        table = {\"R\":\"+\", \"H\":\"+\", \"K\":\"+\", \n",
    "         \"D\":\"-\", \"E\":\"-\", \n",
    "         \"S\":\"P\", \"T\":\"P\", \"N\":\"P\", \"Q\":\"P\", \n",
    "         \"A\":\"H\", \"V\":\"H\", \"I\":\"H\", \"L\":\"H\", \"M\":\"H\", \"F\":\"H\", \"Y\":\"H\", \"W\":\"H\",\n",
    "         \"C\": \"*\", \"G\":\"H\", \"P\":\"H\"}\n",
    "    \n",
    "    elif table ==\"MirnyShakhnovich\": #aliphatic A #aromatic R #polar P #positive + #negative - #hydrophobic H\n",
    "        table = {\"R\":\"+\", \"K\":\"+\", \n",
    "         \"D\":\"-\", \"E\":\"-\", \n",
    "         \"S\":\"P\", \"T\":\"P\", \"N\":\"P\", \"Q\":\"P\",         \n",
    "         \"A\":\"A\", \"V\":\"A\", \"L\":\"A\", \"I\":\"A\", \"M\":\"H\", \"C\":\"A\",\n",
    "         \"F\":\"R\", \"Y\":\"R\", \"W\":\"R\", \"H\":\"R\",\n",
    "         \"G\":\"H\", \"P\":\"H\"}\n",
    "\n",
    "    prop_seq = \"\"\n",
    "    for i in range(0, len(seq)):\n",
    "        aa = seq[i]\n",
    "        prop_seq += table[aa]\n",
    "    return prop_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1162e47d-024e-4bfe-b954-b52cae4a4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_comp(seq, mode):\n",
    "    table = {\"A\":\"T\", \"T\":\"A\", \"C\":\"G\", \"G\":\"C\"}\n",
    "    out = []\n",
    "    if mode == \"rev_comp\" or mode == \"all\":\n",
    "        seq_rev = seq[::-1]\n",
    "        seq_rev_comp = \"\"\n",
    "        for i in range(0,len(seq)):\n",
    "            nt = seq_rev[i]\n",
    "            seq_rev_comp += table[nt]\n",
    "        out.append(seq_rev_comp)\n",
    "\n",
    "    if mode == \"comp\" or mode == \"all\":\n",
    "        seq_rev_comp = \"\"\n",
    "        for i in range(0,len(seq)):\n",
    "            nt = seq[i]\n",
    "            seq_rev_comp += table[nt]\n",
    "        out.append(seq_rev_comp)\n",
    "        \n",
    "    if mode == \"rev\" or mode == \"all\":\n",
    "        seq_rev = seq[::-1]\n",
    "        out.append(seq_rev)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "47206042-a56e-43d3-983b-ecc4b4196d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(seq, frame=0, table=\"standard\", stops=[]):\n",
    "    '''\n",
    "    frame 0-2 = frames 1-3 fwd\n",
    "    frame 3-5 = frames 4-6 rev\n",
    "    frame 6-8 = frames 7-9 comp\n",
    "    frame 9-11 = frames 10-12 rev_comp\n",
    "    frame 12 = all 1-3\n",
    "    frame 13 = all 4-6\n",
    "    frame 14 = all 7-9\n",
    "    frame 15 = all 10-12\n",
    "    frame 16 = all 1-12\n",
    "    '''\n",
    "    \n",
    "    if len(stops)>0:\n",
    "        table_dict = {\"TTT\":\"F\", \"TTC\":\"F\", \"TTA\":\"L\", \"TTG\":\"L\", \"TCT\":\"S\", \"TCC\":\"S\", \"TCA\":\"S\", \"TCG\":\"S\", \"TAT\":\"Y\", \"TAC\":\"Y\",\n",
    "                          \"TAA\":\"Y\", \"TAG\":\"G\", \"TGT\":\"C\", \"TGC\":\"C\", \"TGA\":\"W\", \"TGG\":\"W\", \"CTT\":\"L\", \"CTC\":\"L\", \"CTA\":\"L\", \"CTG\":\"L\",\n",
    "                          \"CCT\":\"P\", \"CCC\":\"P\", \"CCA\":\"P\", \"CCG\":\"P\", \"CAT\":\"H\", \"CAC\":\"H\", \"CAA\":\"Q\", \"CAG\":\"Q\", \"CGT\":\"R\", \"CGC\":\"R\",\n",
    "                          \"CGA\":\"R\", \"CGG\":\"R\", \"ATT\":\"I\", \"ATC\":\"I\", \"ATA\":\"I\", \"ATG\":\"M\", \"ACT\":\"T\", \"ACC\":\"T\", \"ACA\":\"T\", \"ACG\":\"T\",\n",
    "                          \"AAT\":\"N\", \"AAC\":\"N\", \"AAA\":\"K\", \"AAG\":\"K\", \"AGT\":\"S\", \"AGC\":\"S\", \"AGA\":\"R\", \"AGG\":\"R\", \"GTT\":\"V\", \"GTC\":\"V\",\n",
    "                          \"GTA\":\"V\", \"GTG\":\"V\", \"GCT\":\"A\", \"GCC\":\"A\", \"GCA\":\"A\", \"GCG\":\"A\", \"GAT\":\"D\", \"GAC\":\"D\", \"GAA\":\"E\", \"GAG\":\"E\",\n",
    "                          \"GGT\":\"G\", \"GGC\":\"G\", \"GGA\":\"G\", \"GGG\":\"G\"}\n",
    "        for stop in stops:\n",
    "            table_dict[stop] = \"*\"\n",
    "    elif table == \"standard\":\n",
    "        table_dict = {\"TTT\":\"F\", \"TTC\":\"F\", \"TTA\":\"L\", \"TTG\":\"L\", \"TCT\":\"S\", \"TCC\":\"S\", \"TCA\":\"S\", \"TCG\":\"S\", \"TAT\":\"Y\", \"TAC\":\"Y\",\n",
    "                      \"TAA\":\"*\", \"TAG\":\"*\", \"TGT\":\"C\", \"TGC\":\"C\", \"TGA\":\"*\", \"TGG\":\"W\", \"CTT\":\"L\", \"CTC\":\"L\", \"CTA\":\"L\", \"CTG\":\"L\",\n",
    "                      \"CCT\":\"P\", \"CCC\":\"P\", \"CCA\":\"P\", \"CCG\":\"P\", \"CAT\":\"H\", \"CAC\":\"H\", \"CAA\":\"Q\", \"CAG\":\"Q\", \"CGT\":\"R\", \"CGC\":\"R\",\n",
    "                      \"CGA\":\"R\", \"CGG\":\"R\", \"ATT\":\"I\", \"ATC\":\"I\", \"ATA\":\"I\", \"ATG\":\"M\", \"ACT\":\"T\", \"ACC\":\"T\", \"ACA\":\"T\", \"ACG\":\"T\",\n",
    "                      \"AAT\":\"N\", \"AAC\":\"N\", \"AAA\":\"K\", \"AAG\":\"K\", \"AGT\":\"S\", \"AGC\":\"S\", \"AGA\":\"R\", \"AGG\":\"R\", \"GTT\":\"V\", \"GTC\":\"V\",\n",
    "                      \"GTA\":\"V\", \"GTG\":\"V\", \"GCT\":\"A\", \"GCC\":\"A\", \"GCA\":\"A\", \"GCG\":\"A\", \"GAT\":\"D\", \"GAC\":\"D\", \"GAA\":\"E\", \"GAG\":\"E\",\n",
    "                      \"GGT\":\"G\", \"GGC\":\"G\", \"GGA\":\"G\", \"GGG\":\"G\"}\n",
    "    elif table == \"flatworm\":\n",
    "        table_dict = {\"TTT\":\"F\", \"TTC\":\"F\", \"TTA\":\"L\", \"TTG\":\"L\", \"TCT\":\"S\", \"TCC\":\"S\", \"TCA\":\"S\", \"TCG\":\"S\", \"TAT\":\"Y\", \"TAC\":\"Y\",\n",
    "                      \"TAA\":\"Y\", \"TAG\":\"*\", \"TGT\":\"C\", \"TGC\":\"C\", \"TGA\":\"W\", \"TGG\":\"W\", \"CTT\":\"L\", \"CTC\":\"L\", \"CTA\":\"L\", \"CTG\":\"L\",\n",
    "                      \"CCT\":\"P\", \"CCC\":\"P\", \"CCA\":\"P\", \"CCG\":\"P\", \"CAT\":\"H\", \"CAC\":\"H\", \"CAA\":\"Q\", \"CAG\":\"Q\", \"CGT\":\"R\", \"CGC\":\"R\",\n",
    "                      \"CGA\":\"R\", \"CGG\":\"R\", \"ATT\":\"I\", \"ATC\":\"I\", \"ATA\":\"I\", \"ATG\":\"M\", \"ACT\":\"T\", \"ACC\":\"T\", \"ACA\":\"T\", \"ACG\":\"T\",\n",
    "                      \"AAT\":\"N\", \"AAC\":\"N\", \"AAA\":\"N\", \"AAG\":\"K\", \"AGT\":\"S\", \"AGC\":\"S\", \"AGA\":\"S\", \"AGG\":\"S\", \"GTT\":\"V\", \"GTC\":\"V\",\n",
    "                      \"GTA\":\"V\", \"GTG\":\"V\", \"GCT\":\"A\", \"GCC\":\"A\", \"GCA\":\"A\", \"GCG\":\"A\", \"GAT\":\"D\", \"GAC\":\"D\", \"GAA\":\"E\", \"GAG\":\"E\",\n",
    "                      \"GGT\":\"G\", \"GGC\":\"G\", \"GGA\":\"G\", \"GGG\":\"G\"}\n",
    "    elif table == \"TAA\":\n",
    "        table_dict = {\"TTT\":\"F\", \"TTC\":\"F\", \"TTA\":\"L\", \"TTG\":\"L\", \"TCT\":\"S\", \"TCC\":\"S\", \"TCA\":\"S\", \"TCG\":\"S\", \"TAT\":\"Y\", \"TAC\":\"Y\",\n",
    "                      \"TAA\":\"*\", \"TAG\":\"G\", \"TGT\":\"C\", \"TGC\":\"C\", \"TGA\":\"W\", \"TGG\":\"W\", \"CTT\":\"L\", \"CTC\":\"L\", \"CTA\":\"L\", \"CTG\":\"L\",\n",
    "                      \"CCT\":\"P\", \"CCC\":\"P\", \"CCA\":\"P\", \"CCG\":\"P\", \"CAT\":\"H\", \"CAC\":\"H\", \"CAA\":\"Q\", \"CAG\":\"Q\", \"CGT\":\"R\", \"CGC\":\"R\",\n",
    "                      \"CGA\":\"R\", \"CGG\":\"R\", \"ATT\":\"I\", \"ATC\":\"I\", \"ATA\":\"I\", \"ATG\":\"M\", \"ACT\":\"T\", \"ACC\":\"T\", \"ACA\":\"T\", \"ACG\":\"T\",\n",
    "                      \"AAT\":\"N\", \"AAC\":\"N\", \"AAA\":\"N\", \"AAG\":\"K\", \"AGT\":\"S\", \"AGC\":\"S\", \"AGA\":\"S\", \"AGG\":\"S\", \"GTT\":\"V\", \"GTC\":\"V\",\n",
    "                      \"GTA\":\"V\", \"GTG\":\"V\", \"GCT\":\"A\", \"GCC\":\"A\", \"GCA\":\"A\", \"GCG\":\"A\", \"GAT\":\"D\", \"GAC\":\"D\", \"GAA\":\"E\", \"GAG\":\"E\",\n",
    "                      \"GGT\":\"G\", \"GGC\":\"G\", \"GGA\":\"G\", \"GGG\":\"G\"}\n",
    "\n",
    "    prot = \"\"\n",
    "    prot_seq = \"\"\n",
    "    done = \"\"\n",
    "    if frame < 3:\n",
    "        done = \"true\"\n",
    "    elif frame <6:\n",
    "        seq = seq[::-1]\n",
    "        done = \"true\"\n",
    "    elif frame < 9:\n",
    "        seq = rev_comp(seq, \"comp\")[0]\n",
    "        done = \"true\"\n",
    "    elif frame < 12:\n",
    "        seq = rev_comp(seq, \"rev_comp\")[0]\n",
    "        done = \"true\"\n",
    "        \n",
    "    if done == \"true\":\n",
    "        leftover = (len(seq)-frame)%3\n",
    "        for i in range(0+frame, len(seq)-leftover, 3):\n",
    "            codon = seq[i:i+3]\n",
    "            prot += table_dict[codon]\n",
    "    else:\n",
    "        prot = []\n",
    "        if frame < 13:\n",
    "            done = \"true\"\n",
    "        elif frame < 14:\n",
    "            seq = seq[::-1]\n",
    "            done = \"true\"\n",
    "        elif frame < 15:\n",
    "            seq = rev_comp(seq, \"comp\")[0]\n",
    "            done = \"true\"\n",
    "        elif frame < 16:\n",
    "            seq = rev_comp(seq, \"rev_comp\")[0]\n",
    "            done = \"true\"\n",
    "\n",
    "        if done == \"true\":\n",
    "            for j in range(0,3):\n",
    "                prot_seq=\"\"\n",
    "                leftover = (len(seq)-j)%3\n",
    "                for i in range(0+j, len(seq)-leftover, 3):\n",
    "                    codon = seq[i:i+3]\n",
    "                    prot_seq += table_dict[codon]\n",
    "                prot+=[prot_seq]\n",
    "            \n",
    "        elif frame < 17:\n",
    "            for s in [seq, seq[::-1], rev_comp(seq, \"comp\")[0], rev_comp(seq, \"rev_comp\")[0]]:\n",
    "                for j in range(0,3):\n",
    "                    prot_seq=\"\"\n",
    "                    leftover = (len(s)-j)%3\n",
    "                    for i in range(0+j, len(s)-leftover, 3):\n",
    "                        codon = s[i:i+3]\n",
    "                        prot_seq += table_dict[codon]\n",
    "                    prot+=[prot_seq]\n",
    "        else:\n",
    "            print(\"provide a valid frame option\")\n",
    "\n",
    "    return(prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01633bd4-a84a-4de1-9cb9-1fab0c6e0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blast(path, self=False, header=0, sep=\"\\t\"):\n",
    "    if header != 0:\n",
    "        blast_df = pd.read_csv(path, sep=sep, header=header)\n",
    "    else:\n",
    "        blast_df = pd.read_csv(path, sep=sep)\n",
    "    if blast_df.shape[0] != 0:        \n",
    "        blast_df.columns = [\"qseqid\", \"sseqid\", \"slen\", \"qlen\", \"qstart\", \"qend\", \"sstart\", \"send\", \"length\", \"mismatch\", \"gapopen\", \"pident\", \"evalue\", \"bitscore\"]\n",
    "        blast_df[\"sorientation\"] = \"+\"\n",
    "        blast_df.loc[blast_df[\"sstart\"]>blast_df[\"send\"], \"sorientation\"] = \"-\"\n",
    "        temp_df1 = blast_df.copy()\n",
    "        blast_df.loc[blast_df[\"sorientation\"]==\"-\", \"sstart\"] = blast_df[\"send\"]\n",
    "        blast_df.loc[blast_df[\"sorientation\"]==\"-\", \"send\"] = temp_df1[\"sstart\"]\n",
    "    \n",
    "        #filter self-alignment\n",
    "        if self == True:\n",
    "            blast_df = blast_df[(blast_df[\"qstart\"]!=blast_df[\"sstart\"])&(blast_df[\"qend\"]!=blast_df[\"send\"])]\n",
    "            blast_df = blast_df[(blast_df[\"qstart\"]!=blast_df[\"send\"])&(blast_df[\"qend\"]!=blast_df[\"sstart\"])]\n",
    "            blast_df = blast_df[blast_df[\"qstart\"]<blast_df[\"sstart\"]]\n",
    "        \n",
    "        #blast_df = blast_df.groupby([\"qstart\", \"qend\", \"sstart\", \"send\"]).first().reset_index()\n",
    "        \n",
    "        blast_df[\"q_mid\"] = blast_df[\"qend\"]-blast_df[\"qstart\"]\n",
    "        blast_df[\"q_mid\"] = blast_df[\"q_mid\"]/2+blast_df[\"qstart\"]\n",
    "        \n",
    "        blast_df[\"s_mid\"] = blast_df[\"send\"]-blast_df[\"sstart\"]\n",
    "        blast_df[\"s_mid\"] = blast_df[\"s_mid\"]/2+blast_df[\"sstart\"]\n",
    "        \n",
    "        blast_df[\"q_mid\"] = blast_df[\"q_mid\"].astype(int)\n",
    "        blast_df[\"s_mid\"] = blast_df[\"s_mid\"].astype(int)\n",
    "    return(blast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bda294-2359-43c2-b14e-ec39a695358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlaps_wpartners(df, by):\n",
    "\n",
    "    if by == \"q\":\n",
    "        start_col = \"qstart\"\n",
    "        end_col = \"qend\"\n",
    "        start_col_2 = \"sstart\"\n",
    "        end_col_2 = \"send\"\n",
    "    elif by == \"s\":\n",
    "        start_col = \"sstart\"\n",
    "        end_col = \"send\"\n",
    "        start_col_2 = \"qstart\"\n",
    "        end_col_2 = \"qend\"\n",
    "    elif by == \"gtf\":\n",
    "        start_col = \"Start\"\n",
    "        end_col = \"End\"    \n",
    "    elif by ==\"LeftRight\":\n",
    "        start_col = \"Left\"\n",
    "        end_col = \"Right\"\n",
    "        \n",
    "    df = df.sort_values(by=start_col).reset_index(drop=True)\n",
    "    \n",
    "    overlapping1 = []\n",
    "    overlapping_coverage1 = []\n",
    "    partner_coord_groups = []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through the sorted intervals\n",
    "    while i < len(df):\n",
    "        start = df.iloc[i][start_col]\n",
    "        end = df.iloc[i][end_col]\n",
    "        s2 = df.iloc[i][start_col_2]\n",
    "        e2 = df.iloc[i][end_col_2]\n",
    "        \n",
    "        partner_coords = []\n",
    "        partner_coords.append([s2,e2])\n",
    "        \n",
    "        overlap_group = [i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Find overlapping intervals\n",
    "        while i < len(df) and df.iloc[i][start_col] <= end:\n",
    "            end = max(end, df.iloc[i][end_col])\n",
    "            s2 = df.iloc[i][start_col_2]\n",
    "            e2 = df.iloc[i][end_col_2]\n",
    "            partner_coords.append([s2,e2])\n",
    "            overlap_group.append(i)\n",
    "            i += 1\n",
    "\n",
    "        partner_coord_groups.append(partner_coords)\n",
    "        overlapping1.append(overlap_group)\n",
    "        overlapping_coverage1.append([start, end])\n",
    "\n",
    "    # Create the 'overlap_df1' DataFrame\n",
    "    overlap_df1 = pd.DataFrame({\n",
    "        'Transcripts': overlapping1,\n",
    "        'Coords': overlapping_coverage1,\n",
    "        'TrNum': range(len(overlapping1)),\n",
    "        'partner_coords': partner_coord_groups\n",
    "    })\n",
    "    overlap_df1[\"Left\"] = [item[0] for item in overlap_df1[\"Coords\"].values]\n",
    "    overlap_df1[\"Right\"] = [item[1] for item in overlap_df1[\"Coords\"].values]\n",
    "    \n",
    "    return overlap_df1, overlapping_coverage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9978e-cd7f-4897-98ab-faf7ea593879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overlaps(df, by):\n",
    "\n",
    "    if by == \"q\":\n",
    "        start_col = \"qstart\"\n",
    "        end_col = \"qend\"\n",
    "    elif by == \"s\":\n",
    "        start_col = \"sstart\"\n",
    "        end_col = \"send\"\n",
    "    elif by == \"gtf\":\n",
    "        start_col = \"Start\"\n",
    "        end_col = \"End\"    \n",
    "    elif by ==\"LeftRight\":\n",
    "        start_col = \"Left\"\n",
    "        end_col = \"Right\"\n",
    "    else:\n",
    "        start_col = by[0]\n",
    "        end_col = by[1]\n",
    "        \n",
    "    df = df.sort_values(by=start_col).reset_index(drop=True)\n",
    "    \n",
    "    overlapping1 = []\n",
    "    overlapping_coverage1 = []\n",
    "    group_counter = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through the sorted intervals\n",
    "    while i < len(df):\n",
    "        start = df.iloc[i][start_col]\n",
    "        end = df.iloc[i][end_col]\n",
    "        overlap_group = [i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Find overlapping intervals\n",
    "        while i < len(df) and df.iloc[i][start_col] <= end:\n",
    "            end = max(end, df.iloc[i][end_col])\n",
    "            overlap_group.append(i)\n",
    "            i += 1\n",
    "        \n",
    "        overlapping1.append(overlap_group)\n",
    "        overlapping_coverage1.append([start, end])\n",
    "\n",
    "    # Create the 'overlap_df1' DataFrame\n",
    "    overlap_df1 = pd.DataFrame({\n",
    "        'Transcripts': overlapping1,\n",
    "        'Coords': overlapping_coverage1,\n",
    "        'TrNum': range(len(overlapping1))\n",
    "    })\n",
    "    overlap_df1[\"Left\"] = [item[0] for item in overlap_df1[\"Coords\"].values]\n",
    "    overlap_df1[\"Right\"] = [item[1] for item in overlap_df1[\"Coords\"].values]\n",
    "    \n",
    "    return overlap_df1, overlapping_coverage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672d43d-e218-4302-a382-082dcb236f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaps_to_plot(overlap_df1, overlapping_coverage1, seq, yoffset=1, xoffset=0):\n",
    "    empty_regions = []\n",
    "    for i in range(1, len(overlapping_coverage1)):\n",
    "        if overlapping_coverage1[i - 1][1] + 1<overlapping_coverage1[i][0] - 1:\n",
    "            empty_regions.append([overlapping_coverage1[i - 1][1] + 1, overlapping_coverage1[i][0] - 1])\n",
    "\n",
    "        # [11,12][13,14]\n",
    "        # 13,12\n",
    "        # [11,12][14,15]\n",
    "        # 13,13\n",
    "    #empty_regions.append([0, overlapping_coverage1[0][0] - 1])\n",
    "    if overlapping_coverage1[0][0] == 0:\n",
    "        empty_regions.append([-100, overlapping_coverage1[0][0] - 1])\n",
    "    else:\n",
    "        empty_regions.append([0, overlapping_coverage1[0][0] - 1])\n",
    "    if isinstance(seq,str):\n",
    "        if overlapping_coverage1[-1][1] == len(seq):\n",
    "            empty_regions.append([overlapping_coverage1[-1][1] + 1, len(seq)+100])\n",
    "        else:\n",
    "            empty_regions.append([overlapping_coverage1[-1][1] + 1, len(seq)+100])\n",
    "    elif isinstance(seq,int):\n",
    "        if overlapping_coverage1[-1][1] == seq:\n",
    "            empty_regions.append([overlapping_coverage1[-1][1] + 1, seq+100])\n",
    "        else:\n",
    "            empty_regions.append([overlapping_coverage1[-1][1] + 1, seq+100])\n",
    "    \n",
    "    dfdf1 = pd.DataFrame(empty_regions, columns=['Left', 'Right'])\n",
    "    dfdf1['Coverage'] = 0\n",
    "    dfdf3 = pd.concat([dfdf1[\"Left\"], dfdf1[\"Right\"]], axis = 0)\n",
    "    dfdf3 = dfdf3.to_frame(name = \"Coords\")\n",
    "    dfdf3[\"Coverage\"] = 0\n",
    "    dfdf4 = pd.concat([overlap_df1[\"Left\"], overlap_df1[\"Right\"]], axis = 0)  \n",
    "    dfdf4 = dfdf4.to_frame(name = \"Coords\")\n",
    "    dfdf4[\"Coverage\"] = yoffset\n",
    "    tr_cov_df = pd.concat([dfdf3, dfdf4], axis = 0)\n",
    "    tr_cov_df = tr_cov_df.sort_values(by=\"Coords\")\n",
    "    tr_cov_df[\"Coords\"] = tr_cov_df[\"Coords\"]+xoffset\n",
    "    return tr_cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac0335-5b29-4bc3-9c2b-ecf73af6b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtf_to_plot(gtf_df, seq_len):\n",
    "    gtf_df.sort_values(by='Start', inplace=True)\n",
    "    \n",
    "    # Initialize variables\n",
    "    overlapping1 = []\n",
    "    overlapping_coverage1 = []\n",
    "    group_counter = 0\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through the sorted intervals\n",
    "    while i < len(gtf_df):\n",
    "        start = gtf_df.iloc[i]['Start']\n",
    "        end = gtf_df.iloc[i]['End']\n",
    "        overlap_group = [i]\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Find overlapping intervals\n",
    "        while i < len(gtf_df) and gtf_df.iloc[i]['Start'] <= end:\n",
    "            end = max(end, gtf_df.iloc[i]['End'])\n",
    "            overlap_group.append(i)\n",
    "            i += 1\n",
    "        \n",
    "        overlapping1.append(overlap_group)\n",
    "        overlapping_coverage1.append([start, end])\n",
    "    \n",
    "    # Create the 'overlap_df1' DataFrame\n",
    "    overlap_df1 = pd.DataFrame({\n",
    "        'Transcripts': overlapping1,\n",
    "        'Coords': overlapping_coverage1,\n",
    "        'TrNum': range(len(overlapping1))\n",
    "    })\n",
    "    \n",
    "    # Create the 'empty_regions' DataFrame\n",
    "    empty_regions = []\n",
    "    for i in range(1, len(overlapping_coverage1)):\n",
    "        empty_regions.append([overlapping_coverage1[i - 1][1] + 1, overlapping_coverage1[i][0] - 1])\n",
    "    \n",
    "    empty_regions.append([0, overlapping_coverage1[0][0] - 1])\n",
    "    empty_regions.append([overlapping_coverage1[-1][1] + 1, seq_len])\n",
    "    \n",
    "    dfdf1 = pd.DataFrame(empty_regions, columns=['Left', 'Right'])\n",
    "    dfdf1['Coverage'] = 0\n",
    "    dfdf3 = pd.concat([dfdf1[\"Left\"], dfdf1[\"Right\"]], axis = 0)\n",
    "    dfdf3 = dfdf3.to_frame(name = \"Coords\")\n",
    "    dfdf3[\"Coverage\"] = 0\n",
    "    overlap_df1[\"Left\"] = [item[0] for item in overlap_df1[\"Coords\"].values]\n",
    "    overlap_df1[\"Right\"] = [item[1] for item in overlap_df1[\"Coords\"].values]\n",
    "    dfdf4 = pd.concat([overlap_df1[\"Left\"], overlap_df1[\"Right\"]], axis = 0)  \n",
    "    dfdf4 = dfdf4.to_frame(name = \"Coords\")\n",
    "    dfdf4[\"Coverage\"] = 1\n",
    "    tr_cov_df_ = pd.concat([dfdf3, dfdf4], axis = 0)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    return tr_cov_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3ceaa-1127-4cf0-b312-96f0e72035a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta(path, rev_comp=False, rm_barcode=False, pdf=True):\n",
    "    ids = []\n",
    "    seqs = []\n",
    "    seq_df=[]\n",
    "    if pdf:\n",
    "        seq_df = pd.DataFrame()\n",
    "        seq_list = []\n",
    "        record_list = []\n",
    "        \n",
    "        for record in SeqIO.parse(path, \"fasta\"):\n",
    "            record_list.append(record.id)\n",
    "            seq_list.append(str(record.seq))\n",
    "        \n",
    "        seq_df[\"id\"] = record_list\n",
    "        seq_df[\"seq\"] = seq_list\n",
    "        \n",
    "        if rm_barcode:\n",
    "            seq_df[\"seq\"] = seq_df[\"seq\"].apply(lambda x: x[rm_barcode:-rm_barcode])\n",
    "            \n",
    "        if rev_comp == True:\n",
    "            seq_df[\"seq_rev\"] = seq_df[\"seq\"].apply(lambda x: x[::-1])\n",
    "            seq_df[\"seq_rev_comp\"] = seq_df[\"seq\"].apply(lambda x: str(Seq(x).reverse_complement()))\n",
    "            seq_df[\"seq_comp\"] = seq_df[\"seq\"].apply(lambda x: str(Seq(x).complement()))\n",
    "    \n",
    "    else:\n",
    "        file = open(path)\n",
    "        i=1\n",
    "        seqs = []\n",
    "        ids = []\n",
    "        if rm_barcode:\n",
    "            for line in file:\n",
    "                if i%2 == 0:\n",
    "                    seqs.append(line[rm_barcode:-rm_barcode-1])\n",
    "                    i+=1\n",
    "                else:\n",
    "                    ids.append(line[1:-1])\n",
    "                    i+=1\n",
    "        else:\n",
    "            for line in file:\n",
    "                if i%2 == 0:\n",
    "                    seqs.append(line[:-1])\n",
    "                    i+=1\n",
    "                else:\n",
    "                    ids.append(line[1:-1])\n",
    "                    i+=1\n",
    "                \n",
    "    return seq_df, ids, seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4c2c5-c030-45c2-acf4-ba2e658b7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_from_fasta_df(seq_df, seq_col_name, id_col_name, path, rc=False):\n",
    "    write_file = open(path, \"w\")\n",
    "    for index, row in seq_df.iterrows():\n",
    "        write_file.write(\">\" + seq_df[id_col_name][index] + \"\\n\" + seq_df[seq_col_name][index] + \"\\n\")\n",
    "        if rc==True:\n",
    "            write_file.write(\">\" + seq_df[id_col_name][index] + \"_comp\" + \"\\n\" + seq_df[\"seq_comp\"][index] + \"\\n\")\n",
    "            write_file.write(\">\" + seq_df[id_col_name][index] + \"_rev\" + \"\\n\" + seq_df[\"seq_rev\"][index] + \"\\n\")\n",
    "            write_file.write(\">\" + seq_df[id_col_name][index] + \"_rev_comp\"+ \"\\n\" + seq_df[\"seq_rev_comp\"][index] + \"\\n\")\n",
    "    write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e5ab1-2180-4745-beff-6a0ad92a69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_from_seq_list(seq_list, seq_names_list, path):\n",
    "    write_file = open(path, \"w\")\n",
    "    for i in range(len(seq_list)):\n",
    "        write_file.write(\">\" + seq_names_list[i] + \"\\n\" + seq_list[i] + \"\\n\")\n",
    "    write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad8169-d956-4a89-9268-aae5b5859a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_from_MREdf(MREdf, seq, save_path):\n",
    "    write_file = open(save_path, \"w\")\n",
    "    for index, row in MREdf.iterrows():\n",
    "        temp_seq = seq[MREdf[\"MRE_zone_start\"][index]:MREdf[\"MRE_zone_end\"][index]+7]\n",
    "        write_file.write(\">\" + str(MREdf[\"MRE_zone_start\"][index]) + \"-\" + str(MREdf[\"MRE_zone_end\"][index]+7) + \"\\n\" + temp_seq + \"\\n\")\n",
    "    write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a3367-ecb2-4179-b753-103c45454e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_from_overlap_df1(overlap_df1, seq, save_path):\n",
    "    write_file = open(save_path, \"w\")\n",
    "    for index, row in overlap_df1.iterrows():\n",
    "        temp_seq = seq[overlap_df1[\"Left\"][index]:overlap_df1[\"Right\"][index]]\n",
    "        write_file.write(\">\" + str(overlap_df1[\"Left\"][index]) + \"-\" + str(overlap_df1[\"Right\"][index]) + \"\\n\" + temp_seq + \"\\n\")\n",
    "    write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927df420-06f9-4a5a-8296-0a5ead96055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniques_only(data, n=1):\n",
    "    from collections import Counter\n",
    "    histogram = Counter(data)\n",
    "    uniques = [d for d in data if histogram[d] <= n]\n",
    "    return uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90325a7-2b66-4bc3-9216-53db26f2bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniques_only_list(data):\n",
    "    from collections import OrderedDict\n",
    "    list(OrderedDict.fromkeys(t))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb01b7-e258-4814-be5e-9e7916b14e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(list, save_path):\n",
    "    write_list = open(save_path, \"w\")\n",
    "    write_list.write(\"\\n\".join(list))\n",
    "    write_list.write((\"\\n\"))\n",
    "    write_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b2435-668d-4c74-88fa-c940c82c1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list(path):\n",
    "    with open(path)as f:\n",
    "        out=f.read()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "id": "78556b7a-83a0-47a7-8cad-9fc8ff6c1275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_ORFs(seq, stops = [\"TAA\",\"TGA\",\"TAG\"], len_filter=50, to_translate=True):\n",
    "    '''\n",
    "    out: 0 - ORF_coords\n",
    "         1 - ORF_seqs\n",
    "         2 - ORF_stops\n",
    "         3 - ORF_lens\n",
    "         4 - ORF_prots\n",
    "         5 - longest_ORF_coords\n",
    "         6 - num_ORFs_found\n",
    "    '''    \n",
    "    ORF_coords=[]\n",
    "    for j in range(0,3):\n",
    "        leftover = (len(seq)-j)%3\n",
    "        for i in range(0+j, len(seq)-leftover, 3):\n",
    "            codon = seq[i:i+3]\n",
    "            if codon == \"ATG\":\n",
    "                start = i+1\n",
    "                for z in range(i, len(seq)-leftover, 3):\n",
    "                    codon = seq[z:z+3]\n",
    "                    if codon in stops:\n",
    "                        stop = z+3\n",
    "                        ORF_coords.append([start,stop])\n",
    "                        break\n",
    "    ORF_lens = [x[1]-(x[0]-1) for x in ORF_coords]\n",
    "    stops = [seq[x[1]-3:x[1]] for x in ORF_coords]\n",
    "    seqs = [seq[x[0]-1:x[1]] for x in ORF_coords]\n",
    "\n",
    "  \n",
    "    seqs_above_filter_enum = [x for x in enumerate(ORF_lens) if x[1] > len_filter]\n",
    "    longest_ORF_idx = [x[0] for x in seqs_above_filter_enum if x[1]==max(ORF_lens)]\n",
    "    longest_ORF = ORF_coords[longest_ORF_idx[0]]\n",
    "    index = [x[0] for x in seqs_above_filter_enum]\n",
    "    ORFs_above_filter = [ORF_coords[i] for i in index]\n",
    "    stops_above_filter = [stops[i] for i in index]\n",
    "    lens_above_filter = [ORF_lens[i] for i in index]\n",
    "    seqs_above_filter = [seqs[i] for i in index]\n",
    "    num_ORFs_found = len(lens_above_filter)\n",
    "\n",
    "    if to_translate==True:\n",
    "        prots = [translate(x) for x in seqs_above_filter]\n",
    "    else:\n",
    "        prots = None\n",
    "    out = [ORFs_above_filter, seqs_above_filter, stops_above_filter, lens_above_filter, prots, longest_ORF, num_ORFs_found]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fee80785-d793-4906-9137-8dde6e22e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fltc_ORFs(seq, min_len=50, starts=r\"ATG\", stops_list=[\"TAA\",\"TGA\",\"TAG\"], to_translate=True, all_ORFs=False):\n",
    "    import pandas as pd\n",
    "    \n",
    "    ORF_df = []\n",
    "    longest_ORF_coords = []\n",
    "    \n",
    "    i=1\n",
    "    # num_stops = len(stops_list)\n",
    "    # for stop in stops_list:\n",
    "    #     stops_list += stop\n",
    "    #     i+=1\n",
    "    #     if i == num_stops:\n",
    "    #         break\n",
    "    #     else:\n",
    "    #         stops += \"|\"\n",
    "    \n",
    "    stops=\"\"\n",
    "    for s in stops_list:\n",
    "        stops+=s\n",
    "        stops+=\"|\"\n",
    "    stops=stops[:-1]\n",
    "    stops=fr\"{stops}\"\n",
    "\n",
    "    # stops=r\"TAA|TAG|TGA\"\n",
    "    pattern = fr'(?={starts}((?:TGA|TAG|TTT|TTA|TTG|TTC|TCA|TCC|TCT|TCG|TGG|TGT|TGC|TAT|TAC|AAA|AAG|AAC|AAT|AGA|AGG|AGT|AGC|ACA|ACC|ACG|ACT|ATA|ATT|ATC|GGG|GGC|GGT|GGA|GCG|GCC|GCA|GCT|GTA|GTT|GTG|GTC|GAT|GAA|GAG|GAC|CCC|CCA|CCG|CCT|CAC|CAA|CAT|CAG|CTG|CTC|CTT|CTA|CGG|CGC|CGA|CGT)+?)({stops}))'\n",
    "    #ORFs = re.findall(fr'(?=({starts}(?:...)*?)(TAA))', seq)\n",
    "    ORFs = re.findall(pattern, seq)\n",
    "    ORFs_iter = re.finditer(pattern,seq)\n",
    "    ORF_list = []\n",
    "    ORF_len = []\n",
    "    ORF_span = []\n",
    "    ORF_stops = []\n",
    "    ORF_start = []\n",
    "    x=False\n",
    "    j=-1\n",
    "    ORF_idx = []\n",
    "\n",
    "    if len(ORFs) > 0:\n",
    "        for i in ORFs:\n",
    "            j+=1\n",
    "            if len(i[0])>min_len and len(i[0])%3==0:\n",
    "                ORF_list.append(i[0]) \n",
    "                ORF_stops.append(i[1])\n",
    "                ORF_len.append(len(i[0]))\n",
    "                ORF_idx.append(j)\n",
    "                x=True\n",
    "    \n",
    "        z=-1\n",
    "        j=0\n",
    "        if len(ORF_idx) > 0:\n",
    "            for i in ORFs_iter:\n",
    "                z+=1\n",
    "                if z == ORF_idx[j]:\n",
    "                    if j<len(ORF_idx)-1:\n",
    "                        j+=1\n",
    "                    if x == True:\n",
    "                        span = list(i.span())\n",
    "                        start = span[0]+4\n",
    "                        ORF_start += [start]\n",
    "    \n",
    "            ORF_end = []\n",
    "            for i in range(len(ORF_start)):\n",
    "                ORF_end.append(ORF_start[i]+ORF_len[i]-1)\n",
    "        \n",
    "            ORF_df = pd.DataFrame([ORF_list, ORF_stops, ORF_len, ORF_start, ORF_end])\n",
    "            ORF_df = ORF_df.transpose()\n",
    "            ORF_df.columns = [\"seq\", \"stop\", \"seq_len\", \"start_coord\", \"last_coord\"]\n",
    "            ORF_df[\"seq\"] = ORF_df.apply(lambda x: \"ATG\" + x.seq + x.stop, axis=1)\n",
    "            ORF_df[\"start_coord\"] = ORF_df[\"start_coord\"]-3\n",
    "            ORF_df[\"last_coord\"] = ORF_df[\"last_coord\"]+3\n",
    "            ORF_df[\"seq_len\"] = ORF_df[\"seq_len\"]+6\n",
    "            \n",
    "            longest_ORF_idx = ORF_df[\"seq_len\"].idxmax()\n",
    "            longest_ORF_coords = [ORF_df[\"start_coord\"][longest_ORF_idx],ORF_df[\"last_coord\"][longest_ORF_idx]]\n",
    "            if all_ORFs:\n",
    "                ORF_df[\"all_ORF_coords\"] = ORF_df.apply(lambda x: [x.start_coord, x.last_coord],axis=1)\n",
    "                all_ORFs = ORF_df[\"all_ORF_coords\"].tolist()\n",
    "            if to_translate == True:\n",
    "                ORF_df[\"prot\"] = ORF_df[\"seq\"].apply(lambda x: translate(x, stops=stops))\n",
    "        else:\n",
    "            print(\"Discovered ORFs are too short\")\n",
    "    else:\n",
    "        print(\"No ORFs found\")\n",
    "        \n",
    "    num_ORFs_found = len(ORF_df) \n",
    "    return ORF_df, longest_ORF_coords, num_ORFs_found, all_ORFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581f5d3-bd13-406d-82cd-efcf16866129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ORFs_old(seq, min_len=50, starts=\"ATG\", stops=\"TAA|TAG|TGA\"):\n",
    "    ORFs = re.findall(fr'{starts}(.*?)({stops})', seq)\n",
    "    ORF_list = []\n",
    "    for i in ORFs:\n",
    "        if len(i[0])>min_len and len(i[0])%3==0:\n",
    "            ORF = starts + i[0] + i[1]\n",
    "            ORF_list += [ORF]\n",
    "    return ORF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f670b25-6f5f-4e54-97bc-121cf8e3f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every item in the list produces a lists of item indeces\n",
    "def list_duplicates(int_list):\n",
    "    from collections import defaultdict\n",
    "    tally = defaultdict(list)\n",
    "    for i, item in enumerate(int_list):\n",
    "        tally[item].append(i)\n",
    "    return ((key, locs) for key, locs in tally.items()\n",
    "           if len(locs)>1)\n",
    "\n",
    "# for dup in sorted(list_duplicates(all_repeats)):\n",
    "#     print(dup)\n",
    "# to view result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661ed70-42bb-48d3-8ccd-b5729c2de0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_subseqs(fasta_seq, left_right_df):\n",
    "    left_right_df[\"Seq\"] = left_right_df.apply(lambda x: [fasta_seq[x.Left+1, x.Right]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c22978-9dda-4e04-ac93-a4389a356e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nucl_freq(seq_list):\n",
    "    #ACGT x sequence length\n",
    "    #list of sequences as input\n",
    "    from collections import Counter\n",
    "    lines = [list(line[:]) for line in seq_list]\n",
    "    #nucl_frequencies = [[0]*len(lines[0]) for _ in range(4)]\n",
    "    nucl_frequencies = np.zeros([4, len(lines[0])])\n",
    "    lines = np.array(lines)\n",
    "    for i in range(lines.shape[1]):\n",
    "        counter = Counter(lines[:,i])\n",
    "        nuc_types = len(counter)\n",
    "        counter[\"A\"] += 0\n",
    "        counter[\"C\"] += 0\n",
    "        counter[\"G\"] += 0\n",
    "        counter[\"T\"] += 0\n",
    "        j = 0\n",
    "        for key in sorted(counter.keys()):\n",
    "            nucl_frequencies[j][i] = counter[key]/nuc_types\n",
    "            j+=1\n",
    "    return nucl_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a2fd5f-1b29-4123-986b-760c68c8c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aa_freq(seq_list):\n",
    "    #ACDEFGHIKLMNPQRSTVWY x sequence length\n",
    "    #list of sequences as input\n",
    "    from collections import Counter\n",
    "    lines = [list(line[:]) for line in seq_list]\n",
    "    #aa_frequencies = [[0]*len(lines[0]) for _ in range(4)]\n",
    "    aa_frequencies = np.zeros([20, len(lines[0])])\n",
    "    lines = np.array(lines)\n",
    "    for i in range(lines.shape[1]):\n",
    "        counter = Counter(lines[:,i])\n",
    "        aa_types = len(counter)\n",
    "        counter[\"R\"] += 0\n",
    "        counter[\"K\"] += 0\n",
    "        counter[\"D\"] += 0\n",
    "        counter[\"E\"] += 0\n",
    "        counter[\"S\"] += 0\n",
    "        counter[\"T\"] += 0\n",
    "        counter[\"N\"] += 0\n",
    "        counter[\"Q\"] += 0\n",
    "        counter[\"A\"] += 0\n",
    "        counter[\"V\"] += 0\n",
    "        counter[\"L\"] += 0\n",
    "        counter[\"I\"] += 0\n",
    "        counter[\"M\"] += 0\n",
    "        counter[\"C\"] += 0\n",
    "        counter[\"F\"] += 0\n",
    "        counter[\"Y\"] += 0\n",
    "        counter[\"W\"] += 0\n",
    "        counter[\"H\"] += 0\n",
    "        counter[\"G\"] += 0\n",
    "        counter[\"P\"] += 0\n",
    "        j = 0\n",
    "        for key in sorted(counter.keys()):\n",
    "            aa_frequencies[j][i] = counter[key]/aa_types\n",
    "            j+=1\n",
    "    return aa_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a313e73e-0278-4973-be8d-b128c3bc2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_aas(seq):\n",
    "    #ACDEFGHIKLMNPQRSTVWY x sequence length\n",
    "    #list of sequences as input\n",
    "    from collections import Counter\n",
    "    seq = list(seq)\n",
    "    aa_counts = Counter(seq)\n",
    "    return aa_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f68d9a-63e0-433b-8ef5-78570f7bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNA_onehot(seq):\n",
    "    encoded = []\n",
    "    embeddings = {\"C\":[1.,0.,0.,0.], \"G\":[0.,1.,0.,0.], \"A\":[0.,0.,1.,0.], \"T\":[0.,0.,0.,1.]}\n",
    "    for i in seq:\n",
    "        encoded.append(embeddings[i] if i in embeddings.keys() else [0.,0.,0.,0.])\n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba6762b-1530-4257-b309-6fbf6e77470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_kmers(seq, klen, mode=\"DNA\"):\n",
    "    import itertools as it\n",
    "    kmer_dict = {}\n",
    "    if mode == \"DNA\":\n",
    "        DNA = \"ACTGn\"\n",
    "    elif mode == \"protein\":\n",
    "        DNA = \"ACDEFGHIKLMNPQRSTVWY*\"\n",
    "    for output in it.product(DNA,repeat=klen):\n",
    "        kmer_dict[\"\".join(output)]=0\n",
    "    num_kmers = len(seq) - klen +1\n",
    "    for i in range(num_kmers):\n",
    "        kmer=seq[i:i+klen]\n",
    "        if \"N\" not in kmer:\n",
    "            kmer_dict[kmer] +=1\n",
    "    return kmer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f874d272-5f30-4816-9769-9df28771b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs multiple sequences?\n",
    "def find_overrepresented_aas(seq,pseudo_p=0.05):\n",
    "    \n",
    "    import pandas as pd\n",
    "    seq_len = len(seq)\n",
    "    letter_counts = count_kmers(seq,1,mode=\"protein\")\n",
    "    letter_probs = dict()\n",
    "    for key in letter_counts.keys():\n",
    "        letter_probs[key] = letter_counts[key]/seq_len\n",
    "    letter_probs_df = pd.DataFrame()\n",
    "    letter_probs_df[\"letter\"] = letter_counts.keys()\n",
    "    letter_probs_df[\"pseudo_p\"] = pseudo_p\n",
    "    letter_probs_df[\"pseudo_p_count\"] = pseudo_p*len(seq)\n",
    "    letter_probs_df[\"seq_p\"] = letter_probs.values()\n",
    "    letter_probs_df[\"seq_p_count\"] = letter_counts.values()\n",
    "    std_dev = 2+2\n",
    "    std_err = std_dev/math.sqrt(len(seq))\n",
    "    letter_probs_df[\"diff\"] = letter_probs_df[\"seq_p\"]-letter_probs_df[\"pseudo_p\"]\n",
    "    letter_probs_df[\"t_stat\"] = letter_probs_df[\"diff\"]/std_err\n",
    "    \n",
    "    overrepresented_letters = []\n",
    "    return letter_probs_df, overrepresented_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3a52d3b-0e55-411f-9b3f-50265436e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare two populations of data e.g. two sets of sequences\n",
    "def mann_whitney(data1, data2, p=0.05):\n",
    "    import pandas as pd\n",
    "    data_df1 = pd.DataFrame()\n",
    "    data_df2 = pd.DataFrame()\n",
    "    data_df = pd.DataFrame()\n",
    "    data_df1[\"data\"] = data1\n",
    "    data_df2[\"data\"] = data2\n",
    "    data_df1[\"label\"] = 1\n",
    "    data_df2[\"label\"] = 2 \n",
    "    data_df = pd.concat([data_df1, data_df2])\n",
    "    data_df = data_df.sort_values(by=\"data\").reset_index(drop=True)\n",
    "    data_df[\"index\"] = data_df.index\n",
    "    data1_R = data_df[data_df[\"label\"]==1][\"index\"].sum()\n",
    "    data2_R = data_df[data_df[\"label\"]==2][\"index\"].sum()\n",
    "    n1 = data_df1.shape[0]\n",
    "    n2 = data_df2.shape[0]\n",
    "    U1 = n1*n2+(n1*(n1+1))/2 - data1_R\n",
    "    U2 = n2*n1+(n2*(n2+1))/2 - data2_R\n",
    "    U = min(U1,U2)\n",
    "    print(\"n1:\", n1, \"  |  \", \"n2:\", n2)\n",
    "    return(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d344ea-ee38-4e96-8e74-51a9a2a22da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(data1, data2, p=0.05, sample_variances=\"equal\"):\n",
    "    import math\n",
    "    n1 = len(data1)\n",
    "    n2 = len(data2)\n",
    "    deg_freedom = n1+n2-2\n",
    "    sum1 = sum(data1)\n",
    "    sum2 = sum(data2)\n",
    "    mean1 = sum1/n1\n",
    "    mean2 = sum2/n2\n",
    "    mean_diff = abs(mean1-mean2)\n",
    "    \n",
    "    sum_diff1 = 0\n",
    "    for i in data1:\n",
    "        sum_diff1 += (i-mean1)**2\n",
    "    st_dev1 = math.sqrt(sum_diff1/n1)\n",
    "    \n",
    "    sum_diff2 = 0\n",
    "    for j in data2:\n",
    "        sum_diff2 += (i-mean2)**2\n",
    "    st_dev2 = math.sqrt(sum_diff2/n2)\n",
    "    if sample_variances == \"equal\":\n",
    "        pvar = (((n1-1)*st_dev1**2)+((n2-1)*st_dev2**2))/(n1+n2-2)\n",
    "    \n",
    "        t = mean_diff/(math.sqrt(pvar)*math.sqrt(1/n1+1/n2))\n",
    "    elif sample_variances == \"unequal\":\n",
    "        t = mean_diff/math.sqrt(st_dev1**2/n1+st_dev2**2/n2)\n",
    "    return t, deg_freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09dee5ca-d551-4b88-9837-17c275af35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nested_values(it):\n",
    "    if isinstance(it, list):\n",
    "        for sub_it in it:\n",
    "            yield from extract_nested_values(sub_it)\n",
    "    elif isinstance(it, dict):\n",
    "        for value in it.values():\n",
    "            yield from extract_nested_values(value)\n",
    "    else:\n",
    "        yield it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf9f6e-d348-4411-b029-4c704f8e4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_feature_importance(df):\n",
    "    pca = PCA()\n",
    "    x_new = pca.fit_transform(df.values)\n",
    "    \n",
    "    def myplot(score, coeff, labels=None):\n",
    "        xs = score[:,0]\n",
    "        ys = score[:,1]\n",
    "        n=coeff.shape[0]\n",
    "        scalex = 1/(xs.max() - xs.min())\n",
    "        scaley = 1/(ys.max() - ys.min())\n",
    "        plt.scatter(xs*scalex,ys*scaley,c=\"y\")\n",
    "        for i in range(n):\n",
    "            plt.arrow(0, 0, coeff[i,0], coeff[i,1], color= \"r\", alpha=0.5)\n",
    "            if labels is None:\n",
    "                plt.text(coeff[i,0]* 1.15, coeff[i,1] *1.15, \"Var\"+str(i+1), color=\"g\", ha = \"center\", va= \"center\")\n",
    "                Z=0\n",
    "            else:\n",
    "                plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color=\"g\", ha=\"center\", va=\"center\")\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(1))\n",
    "    plt.grid()\n",
    "    \n",
    "    myplot(x_new[:,0:2], np.transpose(pca.components_[0:2, :]))\n",
    "    plt.show()\n",
    "    return x_new, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c18a3-946e-4e5a-8b22-371ed9d4d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tr_ids(tr_ids,to=\"tissue\"):\n",
    "    tr_id_conversion = pd.read_csv(\"/home/maxim/PhD/Project/Data/Long_read_RNAseq/lrRNAseq_barcode_deconvolution/final/blast_final_ids\", header=None)\n",
    "    tr_id_conversion.columns=[\"id\"]\n",
    "    tr_id_conversion[\"id\"]=tr_id_conversion[\"id\"].apply(lambda x: x[1:])\n",
    "    tr_id_conversion[\"id_c\"]=tr_id_conversion[\"id\"].apply(lambda x: re.match(r\"(.*?)(ccs)\",x)[0])\n",
    "\n",
    "    if to==\"tissue\":\n",
    "        tr_converted = tr_id_conversion[tr_id_conversion[\"id_c\"].isin(tr_ids)][\"id\"].tolist()\n",
    "        tr_converted_dict = dict(tr_id_conversion[tr_id_conversion[\"id_c\"].isin(tr_ids)].values)\n",
    "        c_dict = {v: k for k,v in tr_converted_dict.items()}\n",
    "    elif to==\"plain\":\n",
    "        tr_converted = tr_id_conversion[tr_id_conversion[\"id\"].isin(tr_ids)][\"id_c\"].tolist()\n",
    "        tr_converted_dict = dict(tr_id_conversion[tr_id_conversion[\"id\"].isin(tr_ids)].values)\n",
    "        c_dict = {v: k for k,v in tr_converted_dict.items()}\n",
    "    return tr_converted,c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0f407-4bdb-4f9e-b600-28fd06c2ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_GC(seq):\n",
    "    G = seq.count(\"G\")\n",
    "    C = seq.count(\"C\")\n",
    "    seq_len = len(seq)\n",
    "    GC = 100/seq_len*(G+C)\n",
    "    return GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129220f0-95c5-4e82-ab32-4bfc6e807858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process seq subset after removing barcodes\n",
    "def fix_barcode_stat_df(stat_df, unique_id, db=\"Lr13H12\",blast=False):\n",
    "    #forgot to remove barcodes on a small subset of stat df and blast? quickly re-do both here\n",
    "    stat_df_cut = stat_df.copy()\n",
    "    stat_df_cut[\"seq\"] = stat_df_cut[\"seq\"].apply(lambda x: x[39:-39])\n",
    "    if blast:\n",
    "        write_fasta_from_fasta_df(stat_df_cut,\"seq\",\"id\",f\"lrRNAseq_barcode_deconvolution/barcode_fixing/{unique_id}\")\n",
    "        os.system(f\"blastn -query /home/maxim/PhD/Project/Data/Long_read_RNAseq/lrRNAseq_barcode_deconvolution/barcode_fixing/{unique_id} -db /home/maxim/PhD/Project/Data/Sequencing/shinobus_BACs/BLAST_full_BAC/{db}_db -max_hsps 10000 -task blastn -evalue 1e-2 -num_threads 4 -out /home/maxim/PhD/Project/Data/Long_read_RNAseq/lrRNAseq_barcode_deconvolution/barcode_fixing/{unique_id}_blast.out -outfmt '6 qseqid sseqid slen qlen qstart qend sstart send length mismatch gapopen pident evalue bitscore' -word_size 4\")\n",
    "        blast_df = read_blast(f\"/home/maxim/PhD/Project/Data/Long_read_RNAseq/lrRNAseq_barcode_deconvolution/barcode_fixing/{unique_id}_blast.out\")\n",
    "    else:\n",
    "        blast_df=[]\n",
    "    new_stat_df = find_tr_ORF_stats(stat_df_cut[\"seq\"].tolist(), stat_df_cut[\"id\"].tolist(), include_seq=True, tissue=1)\n",
    "    return blast_df, new_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "94c50fb9-8847-407a-83cf-b4d46e946684",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def local_substring_occurrences(seq,subseq,window,skip=0):\n",
    "    #rolling window counter of substrings in local area\n",
    "    from operator import add\n",
    "    seq_len=len(seq)\n",
    "    window_range=seq_len-2*window\n",
    "    counts=[0]*seq_len\n",
    "    if isinstance(subseq,list):\n",
    "        for ss in subseq:\n",
    "            c=[]\n",
    "            for i in range(window):\n",
    "                sample_seq=seq[:i]\n",
    "                count = substring_occurrences(sample_seq,ss)\n",
    "                c.append(count)\n",
    "    \n",
    "            i=0\n",
    "            while i<window_range:\n",
    "                if skip:\n",
    "                    if i%skip==0:\n",
    "                        i+=1\n",
    "                sample_seq = seq[i:i+window]\n",
    "                count = substring_occurrences(sample_seq,ss)\n",
    "                c.append(count)\n",
    "                i+=1\n",
    "        \n",
    "            for i in range(window,0,-1):\n",
    "                sample_seq=seq[-i:]\n",
    "                count = substring_occurrences(sample_seq,ss)\n",
    "                c.append(count)\n",
    "            counts=list(map(add,counts,c))\n",
    "    else:\n",
    "        for i in range(window):\n",
    "            sample_seq=seq[:i]\n",
    "            count = substring_occurrences(sample_seq,subseq)\n",
    "            counts.append(count)\n",
    "    \n",
    "        i=0\n",
    "        while i<window_range:\n",
    "            if skip:\n",
    "                if i%skip==0:\n",
    "                    i+=1\n",
    "            sample_seq = seq[i:i+window]\n",
    "            count = substring_occurrences(sample_seq,subseq)\n",
    "            counts.append(count)\n",
    "            i+=1\n",
    "    \n",
    "        for i in range(window,0,-1):\n",
    "            sample_seq=seq[-i:]\n",
    "            count = substring_occurrences(sample_seq,subseq)\n",
    "            counts.append(count)\n",
    "            \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a71b7-04fd-4b33-ba95-05cadd2c025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranges_over_depth_threshold(depth, coords, depth_threshold, df=False):\n",
    "    \n",
    "    depth_enum = [x for x in enumerate(depth) if x[1] > depth_threshold]\n",
    "    depth_index = [i[0] for i in depth_enum]\n",
    "    desired_depth = [i[1] for i in depth_enum]\n",
    "    desired_coords = [coords[i] for i in depth_index]\n",
    "    \n",
    "    high_depth_ranges=[]\n",
    "    for i in range(len(desired_coords)-1):\n",
    "        if i == 1:\n",
    "            start = desired_coords[i]\n",
    "        if desired_coords[i+1] != desired_coords[i]+1:\n",
    "            end = desired_coords[i]\n",
    "            high_depth_ranges.append([start,end])\n",
    "            start = desired_coords[i+1]\n",
    "    end=desired_coords[i+1]\n",
    "    high_depth_ranges.append([start,end])\n",
    "\n",
    "    if df==True:\n",
    "        high_depth_ranges = pd.DataFrame(high_depth_ranges)\n",
    "        high_depth_ranges.columns=[\"Start\",\"End\"]\n",
    "    \n",
    "    return high_depth_ranges, desired_depth, desired_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0fa47e-ce90-4d79-9b3d-f855e59d6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subseqs_by_coords(seq, coords=[], left=[], right=[]):\n",
    "    subseq_list = []\n",
    "    if len(coords)>0:\n",
    "        for i in coords:\n",
    "            subseq_list.append(seq[i[0]-1:i[1]])\n",
    "            \n",
    "        x=1+1\n",
    "    elif len(left)>0:\n",
    "        for i in range(len(left)):\n",
    "            subseq_list.append(seq[left[i]-1:right[i]])\n",
    "    return subseq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628468c0-ad0d-470f-bb33-b48c566f9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subseqs_by_depth_threshold(seq, depth, coords, depth_threshold):\n",
    "    r, dd, dc = ranges_over_depth_threshold(depth, coords, depth_threshold)\n",
    "    subseq_list = extract_subseqs_by_coords(seq, r)\n",
    "    return subseq_list,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54a4e60-6556-4e5e-9f6c-c9ab3213a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_all_pretty(seq, display_seq=True, stops=[]):\n",
    "    if display_seq:\n",
    "        print(\"seq\",seq)\n",
    "    if stops:\n",
    "        t = translate(seq, frame=16, stops=stops)\n",
    "    else:\n",
    "        t = translate(seq, frame=16)\n",
    "    seq_states = [\"fwd\", \"rev\", \"comp\", \"rev_comp\"]\n",
    "    for i in range(len(t)):\n",
    "        curr_seq_state = int((i)/3)\n",
    "        print(\"frame:\", i+1, \"- \", seq_states[curr_seq_state])\n",
    "        print(t[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fbc48-2591-4461-90c0-d45629412d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_high_depth_regions(seq, depth, coords, depth_threshold):\n",
    "    s,r=subseqs_by_depth_threshold(seq, depth, coords, depth_threshold)\n",
    "    for i in range(1,len(s)+1):\n",
    "        print(\"seq\", i, \"-\", s[i-1])\n",
    "        translate_all_pretty(s[i-1], display_seq=False)\n",
    "        print(\"\\n =========================== \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "c18f7939-d583-4a4c-936e-15e7aa74cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudorandom_gen(seq_len, CGAT_p=[25,25,25,25]):\n",
    "    import numpy as np\n",
    "    sum_p = sum(CGAT_p)\n",
    "    seq_gen = list(np.random.randint(1,sum_p+1, seq_len))\n",
    "    pseudoseq=\"\"\n",
    "    c = CGAT_p[0]\n",
    "    g = CGAT_p[1]+c\n",
    "    a = CGAT_p[2]+g\n",
    "    t = CGAT_p[3]+a\n",
    "    for x in seq_gen:\n",
    "        if x<=c:\n",
    "            pseudoseq = pseudoseq + \"C\"\n",
    "        elif x>c and x <=g:\n",
    "            pseudoseq = pseudoseq + \"G\"\n",
    "        elif x>g and x <=a:\n",
    "            pseudoseq = pseudoseq + \"A\"\n",
    "        else:\n",
    "            pseudoseq = pseudoseq + \"T\"\n",
    "    return pseudoseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a501185c-6a84-4559-9163-42a9fc75100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetitiveness_index(seq, longest_seq_len=10000):\n",
    "    k=4\n",
    "    i=1\n",
    "    while k < longest_seq_len:\n",
    "        k=4\n",
    "        k=k**i\n",
    "        i+=1\n",
    "        # print(i)\n",
    "    kmer_count_range = []\n",
    "\n",
    "    for j in range(i):\n",
    "        kmer_count_range.append(4**j)\n",
    "    # print(kmer_count_range)\n",
    "    seq_len = len(seq)\n",
    "    # print(seq_len)\n",
    "    for z in range(1,i):\n",
    "        if seq_len>=kmer_count_range[z-1] and seq_len<=kmer_count_range[z]:\n",
    "            klen=z\n",
    "            \n",
    "    kmer_ceiling = 4**klen\n",
    "    max_kmers = seq_len-klen+1\n",
    "    kmer_dict = count_kmers1(seq, klen)\n",
    "    kmer_num = len(kmer_dict.keys())\n",
    "    kmer_uniqueness = 100/max_kmers*kmer_nm\n",
    "    kmer_uniqueness_len = 100/max_kmers*kmer_num*seq_len\n",
    "    kmer_uniqueness_klen = 100/max_kmers*kmer_num*(4**klen)\n",
    "    unbound_uniqueness = 100/kmer_ceiling*kmer_num\n",
    "    unbound_uniqueness_len = 100/kmer_ceiling*kmer_num*seq_len\n",
    "    unbound_uniqueness_klen = 100/kmer_ceiling*kmer_num*(4**klen)\n",
    "    return kmer_uniqueness, kmer_uniqueness_len, kmer_uniqueness_klen, unbound_uniqueness, unbound_uniqueness_len, unbound_uniqueness_klen, kmer_ceiling, max_kmers, kmer_num, klen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07463a0-8da0-4067-97e1-a804746b41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetitiveness_index_full_scale(seq, klen):\n",
    "    seq_len = len(seq)\n",
    "    \n",
    "    kmer_ceiling = 4**klen\n",
    "    max_kmers = seq_len-klen+1\n",
    "    kmer_dict = count_kmers1(seq, klen)\n",
    "    kmer_num = len(kmer_dict.keys())\n",
    "    kmer_uniqueness = 100/max_kmers*kmer_num\n",
    "    kmer_uniqueness_len = 100/max_kmers*kmer_num*seq_len\n",
    "    kmer_uniqueness_klen = 100/max_kmers*kmer_num*(4**klen)\n",
    "    unbound_uniqueness = 100/kmer_ceiling*kmer_num\n",
    "    unbound_uniqueness_len = 100/kmer_ceiling*kmer_num*seq_len\n",
    "    unbound_uniqueness_klen = 100/kmer_ceiling*kmer_num*(4**klen)\n",
    "    return kmer_uniqueness, kmer_uniqueness_len, kmer_uniqueness_klen, unbound_uniqueness, unbound_uniqueness_len, unbound_uniqueness_klen, kmer_ceiling, max_kmers, kmer_num, klen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4e58c-e497-44e1-9af3-dd7dd9cb06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetitiveness_index_accumulative(seq, klen, previous=0, prev_len=0):\n",
    "    seq_len = len(seq)\n",
    "    \n",
    "    max_kmers = (seq_len-klen+1)+prev_len\n",
    "    kmer_dict = count_kmers1(seq, klen)\n",
    "    if previous != 0:\n",
    "        for k in previous.keys():\n",
    "            if k in kmer_dict.keys():\n",
    "                kmer_dict[k] += previous[k]\n",
    "            else:\n",
    "                kmer_dict[k] = previous[k]\n",
    "    kmer_num = len(kmer_dict.keys())\n",
    "    kmer_uniqueness = 100/max_kmers*kmer_num\n",
    "    return kmer_uniqueness, max_kmers, kmer_num, kmer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetitiveness_index_ratio_corrected(seq, klen):\n",
    "    seq_len = len(seq)\n",
    "    \n",
    "    kmer_ceiling = 4**klen\n",
    "    max_kmers = seq_len-klen+1\n",
    "    kmer_dict = count_kmers1(seq, klen)\n",
    "    kmer_num = len(kmer_dict.keys())\n",
    "    kmer_uniqueness = 100/max_kmers*kmer_num\n",
    "    kmer_num_scaled = kmer_num/max_kmers*kmer_ceiling\n",
    "    return kmer_num_scaled, kmer_uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6933be-27a7-4f0b-b2fb-d48174ce2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_fasta_blast(blast_path, fasta_path=\"\", span=300000, extract=False, raw_path=\"\", work_dir=\"\"):\n",
    "    if extract:\n",
    "        blast = read_blast(blast_path)\n",
    "        blast[\"soffset\"] = 0\n",
    "        blast.loc[(blast[\"slen\"]>span)&(blast[\"sstart\"]>span/2), \"soffset\"] = blast[\"sstart\"]-(span/2)\n",
    "        blast.loc[(blast[\"slen\"]>span)&(blast[\"sstart\"]>span/2), \"send\"] = blast[\"send\"]-blast[\"soffset\"]\n",
    "        blast.loc[(blast[\"slen\"]>span)&(blast[\"sstart\"]>span/2), \"sstart\"] = span/2\n",
    "        blast[\"slen_init\"] = blast[\"slen\"]\n",
    "        blast.loc[blast[\"slen\"]>span, \"slen\"] = span\n",
    "        os.chdir(work_dir)\n",
    "        ids_list = blast.sort_values(\"slen\", ascending=False).drop_duplicates(\"sseqid\")[\"sseqid\"].astype(str).tolist()\n",
    "        write_list(ids_list, \"ids_ordered.txt\")\n",
    "        blast_fasta = blast.sort_values(\"sseqid\").drop_duplicates(\"sseqid\").reset_index(drop=True)\n",
    "        raw_fasta_path = work_dir+\"/raw_seqs.fasta\"\n",
    "        os.system(f\"seqtk subseq {raw_path} {\"ids_ordered.txt\"} > {raw_fasta_path}\")\n",
    "        fasta_df = parse_fasta(raw_fasta_path)[0]\n",
    "        fasta_df[\"soffset\"] = blast_fasta[\"soffset\"]\n",
    "        fasta_df[\"slen_init\"] = blast_fasta[\"slen_init\"]\n",
    "        fasta_df[\"seq_red\"] = fasta_df[\"seq\"]\n",
    "        fasta_df.loc[fasta_df[\"soffset\"]!=0,\"seq_red\"] = fasta_df.apply(lambda x: x.seq[int(x.soffset-span/2):int(x.soffset+span/2)], axis=1)\n",
    "        fasta_df.loc[fasta_df[\"soffset\"]==0,\"seq_red\"] = fasta_df.apply(lambda x: x.seq[0:span], axis=1)\n",
    "        fasta_df = fasta_df.sort_values(by=\"slen_init\", ascending=False)\n",
    "        fasta_path = work_dir + \"/red_seqs.fasta\"\n",
    "        write_fasta_from_fasta_df(fasta_df, \"seq_red\", \"id\", fasta_path)\n",
    "        red_blast_path = work_dir + \"/red_blast.out\"\n",
    "        blast = blast.sort_values(by=\"slen_init\",ascending=False)\n",
    "        blast.to_csv(red_blast_path, sep=\"\\t\", index=False)\n",
    "        return fasta_df, blast, ids_list,blast_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "074eb67f-2f49-40ff-b609-b1fb5f64487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_with_order(values, make_equal=False, make_sequential=False):\n",
    "    '''\n",
    "    Takes a list of values, sorts the values in ascending order,\n",
    "    keeps track of the changes in original order of items in a list.\n",
    "    Returns list of sorted values and a list of item order before sorting.\n",
    "    '''\n",
    "    enum = list(enumerate(values))\n",
    "    order = [x[0] for x in enum]\n",
    "    values = [x[1] for x in enum]\n",
    "    old_values = [0]*len(values)\n",
    "    ori_order = order.copy()\n",
    "\n",
    "    #loops over values, if value list[i] is greater than value list[i+1], swaps their position and tracks original order\n",
    "    while values != old_values:\n",
    "        old_values = values.copy()\n",
    "        for i in range(len(values)-1):\n",
    "            if values[i]>values[i+1]:\n",
    "                old = values[i]\n",
    "                old_o = order[i]\n",
    "                values[i] = values[i+1]\n",
    "                order[i] = order[i+1]\n",
    "                values[i+1]=old\n",
    "                order[i+1]=old_o\n",
    "                \n",
    "    new_order = order.copy()\n",
    "    idxs = []\n",
    "    idxs_all = []\n",
    "    uniq_values = []\n",
    "    idxs_all_old = []\n",
    "    \n",
    "    for x in values:\n",
    "        if x not in uniq_values:\n",
    "            uniq_values.append(x)\n",
    "\n",
    "    #loops over unique values, records index of each value occurence, sets order of each value to the minimal index \n",
    "    if make_equal:\n",
    "        for i in uniq_values:\n",
    "            idxs=[]\n",
    "            for j in range(len(values)):\n",
    "                if i==values[j]:\n",
    "                    idxs.append(order[j])\n",
    "            min_idx = min(idxs)\n",
    "            dup_num = len(idxs)\n",
    "            for idx in idxs:\n",
    "                new_order[idx]=min_idx\n",
    "                idxs_all.append(min_idx)\n",
    "        order = idxs_all.copy()\n",
    "\n",
    "    if make_sequential:\n",
    "        for i in uniq_values:\n",
    "            idxs=[]\n",
    "            for j in range(len(values)):\n",
    "                if i==values[j]:\n",
    "                    idxs.append(order[j])\n",
    "            min_idx = min(idxs)\n",
    "            dup_num = len(idxs)\n",
    "            for idx in idxs:\n",
    "                new_order[idx]=min_idx\n",
    "                idxs_all.append(min_idx)\n",
    "        \n",
    "        num_uniq = len(uniq_values)-1\n",
    "\n",
    "        idxs_all_old = idxs_all.copy()\n",
    "        \n",
    "        existing=1\n",
    "        \n",
    "        while max(idxs_all)!=num_uniq:\n",
    "            for i in range(len(idxs_all)):\n",
    "                if i not in idxs_all and existing==1:\n",
    "                    set_to = i\n",
    "                    # print(set_to)\n",
    "                    existing=0\n",
    "                if existing==0:\n",
    "                    # print(i)\n",
    "                    for j in range(len(idxs_all)):\n",
    "                        if idxs_all[j] == i:\n",
    "                            idxs_all[j] = set_to\n",
    "                            existing=1\n",
    "                            # print(j)\n",
    "            existing=1\n",
    "            \n",
    "        order = idxs_all.copy()\n",
    "        \n",
    "    return values, order\n",
    "    # return values, order, new_order, idxs, idxs_all_old, idxs_all, uniq_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cc163bf-f126-4d97-8b63-56337c95f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_with_order(values, make_equal=False, make_sequential=False):\n",
    "    '''\n",
    "    Takes a list of values, sorts the values in ascending order,\n",
    "    keeps track of the changes in original order of items in a list.\n",
    "    Returns list of sorted values and a list of item order before sorting.\n",
    "    '''\n",
    "    enum = list(enumerate(values))\n",
    "    order = [x[0] for x in enum]\n",
    "    values = [x[1] for x in enum]\n",
    "    old_values = [0]*len(values)\n",
    "    ori_order = order.copy()\n",
    "\n",
    "    #loops over values, if value list[i] is greater than value list[i+1], swaps their position and tracks original order\n",
    "    while values != old_values:\n",
    "        old_values = values.copy()\n",
    "        for i in range(len(values)-1):\n",
    "            if values[i]>values[i+1]:\n",
    "                old = values[i]\n",
    "                old_o = order[i]\n",
    "                values[i] = values[i+1]\n",
    "                order[i] = order[i+1]\n",
    "                values[i+1]=old\n",
    "                order[i+1]=old_o\n",
    "                \n",
    "    new_order = order.copy()\n",
    "    idxs = []\n",
    "    idxs_all = []\n",
    "    uniq_values = []\n",
    "    idxs_all_old = []\n",
    "    \n",
    "    for x in values:\n",
    "        if x not in uniq_values:\n",
    "            uniq_values.append(x)\n",
    "\n",
    "    #loops over unique values, records index of each value occurence, sets order of each value to the minimal index \n",
    "    if make_equal:\n",
    "        for i in uniq_values:\n",
    "            idxs=[]\n",
    "            for j in range(len(values)):\n",
    "                if i==values[j]:\n",
    "                    idxs.append(order[j])\n",
    "            min_idx = min(idxs)\n",
    "            dup_num = len(idxs)\n",
    "            for idx in idxs:\n",
    "                new_order[idx]=min_idx\n",
    "                idxs_all.append(min_idx)\n",
    "        order = idxs_all.copy()\n",
    "\n",
    "    if make_sequential:\n",
    "        for i in uniq_values:\n",
    "            idxs=[]\n",
    "            for j in range(len(values)):\n",
    "                if i==values[j]:\n",
    "                    idxs.append(order[j])\n",
    "            min_idx = min(idxs)\n",
    "            dup_num = len(idxs)\n",
    "            for idx in idxs:\n",
    "                new_order[idx]=min_idx\n",
    "                idxs_all.append(min_idx)\n",
    "        \n",
    "        num_uniq = len(uniq_values)-1\n",
    "\n",
    "        idxs_all_old = idxs_all.copy()\n",
    "        \n",
    "        existing=1\n",
    "        \n",
    "        while max(idxs_all)!=num_uniq:\n",
    "            for i in range(len(idxs_all)):\n",
    "                if i not in idxs_all and existing==1:\n",
    "                    set_to = i\n",
    "                    # print(set_to)\n",
    "                    existing=0\n",
    "                if existing==0:\n",
    "                    # print(i)\n",
    "                    for j in range(len(idxs_all)):\n",
    "                        if idxs_all[j] == i:\n",
    "                            idxs_all[j] = set_to\n",
    "                            existing=1\n",
    "                            # print(j)\n",
    "            existing=1\n",
    "            \n",
    "        order = idxs_all.copy()\n",
    "        \n",
    "    return values, order\n",
    "    # return values, order, new_order, idxs, idxs_all_old, idxs_all, uniq_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f3f6e95-a29b-4281-9432-5f6c0ae9685c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 600, 600], [0, 2, 1, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values = [1,600,1,600]\n",
    "# # values = [2,35, 5, 35,12,33,4]\n",
    "# sv, order = sort_with_order(values)\n",
    "# sort_with_order(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb61c52d-21dc-4549-916a-1d5b33e9df82",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2664944077.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[33], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# uniq_values = []\n",
    "\n",
    "# for x in values:\n",
    "#     if x not in uniq_values:\n",
    "#         uniq_values.append(x)\n",
    "\n",
    "# for i in uniq_values:\n",
    "#     idxs=[]\n",
    "#     for j in range(len(values)):\n",
    "#         if i==values[j]:\n",
    "#             idxs.append(order[j])\n",
    "#     min_idx = min(idxs)\n",
    "#     dup_num = len(idxs)\n",
    "#     for idx in idxs:\n",
    "#         new_order[idx]=min_idx\n",
    "#         idxs_all.append(min_idx)\n",
    "# order = idxs_all.copy()\n",
    "\n",
    "\n",
    "# '''\n",
    "# create list of [0] * len(order)\n",
    "# populate indexes of list with the minimal index value of order items by index - similar to make_equal, but applied to original values order\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fae6716-4c58-45d5-8084-aca440fb9fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 600, 600], [0, 0, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort_with_order(values, make_equal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a964cf1-96f4-49d7-b857-a659ec42a752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 600, 600], [0, 0, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sv1, o1 = sort_with_order(values,make_sequential=1)\n",
    "# sort_with_order(values,make_sequential=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dbe7c-6074-4bb3-ba59-13404a1f88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(sv)):\n",
    "#     val = sv[i]\n",
    "#     for j in range(len(sv)):\n",
    "#         if val == sv1[j]:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d222e352-09cc-4bca-9878-b4eaa8875bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_order(order):\n",
    "    '''\n",
    "    #outputs sequential list of values, ascending in value every time an input list value changes\n",
    "    '''\n",
    "    new_order = []\n",
    "    uniques = get_unique(order)\n",
    "    for i in range(len(uniques)):\n",
    "        for j in range(len(order)):\n",
    "            if order[j]==uniques[i]:\n",
    "                new_order.append(i)\n",
    "                \n",
    "    return new_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23855666-9e1d-445a-9c9c-c60a9a699d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(values_list):\n",
    "    \n",
    "    uniq_values=[]\n",
    "    for x in values_list:\n",
    "        if x not in uniq_values:\n",
    "            uniq_values.append(x)\n",
    "            \n",
    "    return uniq_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f076ba-03b6-4f1f-802f-1518e9427fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_elements(list_x, spacing=250):\n",
    "    for i in range(len(list_x)-1):\n",
    "        if list_x[i+1]-list_x[i] < spacing:\n",
    "            list_x[i+1]=list_x[i]+spacing\n",
    "    return list_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051725a3-9626-47da-a67a-0d5eafcdd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntenous_indexes(synteny_list):\n",
    "    structure = []\n",
    "    start=1\n",
    "    for i in range(len(synteny_list)-1):\n",
    "        if synteny_list[i+1]>= synteny_list[i] and start!=-1:\n",
    "            start = -1\n",
    "            structure.append(i)\n",
    "        elif synteny_list[i+1]<synteny_list[i]:\n",
    "            structure.append(i)\n",
    "            start=1\n",
    "    structure.append(len(synteny_list)-1)\n",
    "    return structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
